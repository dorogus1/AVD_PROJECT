{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install tf-playwright-stealth",
   "id": "42a4b0e160b61d19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# EUvsDISINFO Scraper\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from playwright_stealth import stealth_async        # Import stealth_async for stealth mode in order to avoid detection\n",
    "\n",
    "nest_asyncio.apply()  # Apply nest_asyncio to allow nested event loops\n",
    "\n",
    "async def scrape_eu_vs_disinfo():\n",
    "    pw = None\n",
    "    browser = None\n",
    "    results = []  # Initialize an empty list to store results\n",
    "    try:\n",
    "        pw = await async_playwright().start()  # Start Playwright\n",
    "        browser = await pw.chromium.launch(headless=False)  # Launch Chromium browser in non-headless mode because the website detects headless browsers\n",
    "        page = await browser.new_page()  # Create a new page in the browser\n",
    "\n",
    "\n",
    "        await stealth_async(page)  # Apply stealth mode to avoid detection\n",
    "        url = f\"https://euvsdisinfo.eu/disinformation-cases/page/1/?disinfo_countries%5B0%5D=country_77544&_=1750026467335\"\n",
    "        await page.goto(url, timeout=120000, wait_until='domcontentloaded')  # Navigate to the page\n",
    "        await asyncio.sleep(1)  # Small delay for ethical scraping\n",
    "\n",
    "        pagination_tags = await page.query_selector('div.b-pagination a:last-child')  # Select last pagination item\n",
    "        num_pages = await pagination_tags.inner_text()  # Get the number of pages\n",
    "        print(f\"Number of pages: {num_pages}\")\n",
    "        num_pages = int(num_pages)\n",
    "\n",
    "        for page_num in range(1, num_pages + 1):  # Iterating through web pages\n",
    "            if page_num > 1:\n",
    "\n",
    "                await stealth_async(page)  # Apply stealth mode to avoid detection\n",
    "                print(f\"Navigating to page {page_num}...\")\n",
    "                url = f\"https://euvsdisinfo.eu/disinformation-cases/page/{page_num}/?disinfo_countries%5B0%5D=country_77544&_=1750026467335\"\n",
    "                await page.goto(url, timeout=120000, wait_until='domcontentloaded')  # Navigate to the page\n",
    "                print(f\"Page {page_num} loaded.\")\n",
    "                await asyncio.sleep(1)  # Small delay for ethical scraping\n",
    "\n",
    "            article_cards = await page.query_selector_all('a.b-archive__database-item') # Select all article cards on the page\n",
    "\n",
    "            # Extract data from the first few cards\n",
    "            for i, card in enumerate(article_cards):\n",
    "                outlet = []  # Initialize an empty list for outlets\n",
    "                tags = []  # Initialize an empty list for tags\n",
    "                title = \"N/A\"  # Initialize title as \"N/A\"\n",
    "                summary = \"N/A\"  # Initialize summary as \"N/A\"\n",
    "                response = \"N/A\"  # Initialize response as \"N/A\"\n",
    "                date = \"N/A\"  # Initialize date as \"N/A\"\n",
    "                languages = []  # Initialize an empty list for languages\n",
    "                countries = []  # Initialize an empty list for countries\n",
    "\n",
    "                url = await card.get_attribute('href') # Get the URL of the article\n",
    "\n",
    "                article_page = await browser.new_page()  # Create a new page for article\n",
    "\n",
    "\n",
    "                await stealth_async(article_page)  # Apply stealth mode to avoid detection\n",
    "                await article_page.goto(f\"https://euvsdisinfo.eu{url}\", timeout=120000, wait_until='domcontentloaded')  # Navigate to the article page\n",
    "                await asyncio.sleep(1) # Small delay for ethical scraping\n",
    "\n",
    "                try:\n",
    "                    await article_page.wait_for_selector('h1.b-page__title', timeout=100)  # Wait for the title to load\n",
    "                except Exception as e:\n",
    "                    title_tag = await article_page.query_selector('h1.b-report__title')  # Select the title of the article\n",
    "                    title = await title_tag.inner_text() if title_tag else \"N/A\"  # Get the title text\n",
    "                    title = title.replace(\"DISINFO: \", \"\")  # Clean the title text\n",
    "\n",
    "                    summary_tag = await article_page.query_selector('div.b-report__summary')  # Select the summary of the article\n",
    "                    summary = await summary_tag.inner_text() if summary_tag else \"N/A\"  # Get the summary text\n",
    "                    summary = summary.replace(\"SUMMARY\\n\\n\", \"\")  # Clean the summary text\n",
    "\n",
    "                    response_tag = await article_page.query_selector('div.b-report__response')  # Select the response of the article\n",
    "                    response = await response_tag.inner_text() if response_tag else \"N/A\"  # Get the response text\n",
    "                    response = response.replace(\"RESPONSE\\n\\n\", \"\")  # Clean the response text\n",
    "\n",
    "                    details = await article_page.query_selector_all('ul.b-report__details-list li')  # Select the details of the article\n",
    "\n",
    "                    if len(details) == 2:\n",
    "                        date_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(1) span')  # Select the date of the article\n",
    "                        date = await date_tag.inner_text() if date_tag else \"N/A\"  # Get the date text\n",
    "\n",
    "                        countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(2) span')  # Select the countries of the article\n",
    "                        countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                        countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "                    elif len(details) >= 3:\n",
    "                        outlet_tag = await article_page.query_selector_all('ul.b-report__details-list li:nth-child(1) a')  # Select the outlets of the article\n",
    "                        for j, outlet_item in enumerate(outlet_tag):  # Iterate through each outlet\n",
    "                            outlet_text = await outlet_item.inner_text()  # Get the outlet text\n",
    "                            if outlet_text and not outlet_text.strip().__contains__(\"archived\"):  # Clean the outlet text and append it to the outlet list\n",
    "                                outlet.append(outlet_text.strip().replace(\"\\\\n(opens in a new tab)\", \"\").replace(\"\\n(opens in a new tab)\", \"\"))\n",
    "\n",
    "                        date_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(2) span')  # Select the date of the article\n",
    "                        date = await date_tag.inner_text() if date_tag else \"N/A\"  # Get the date text\n",
    "\n",
    "                        if len(details) == 4:\n",
    "                            languages_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(3) span')  # Select the languages of the article\n",
    "                            languages = [await languages_tag.inner_text()]  # Get the languages text\n",
    "                            languages = languages[0].split(\", \") if languages else []  # Split the languages text into a list\n",
    "                            languages = list(set(languages))  # Remove duplicates\n",
    "\n",
    "                            countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(4) span')  # Select the countries of the article\n",
    "                            countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                            countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "                        elif len(details) == 3:\n",
    "                            languages = []\n",
    "\n",
    "                            countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(3) span')  # Select the countries of the article\n",
    "                            countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                            countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "\n",
    "                    tags_tag = await article_page.query_selector_all('div.b-report__keywords a span')  # Select the tags of the article\n",
    "                    for tag_item in tags_tag:  # Iterate through each tag\n",
    "                        tag_text = await tag_item.inner_text()  # Get the tag text\n",
    "                        if tag_text:\n",
    "                            tags.append(tag_text.strip())  # Append the tag text to the tags list\n",
    "\n",
    "                    results.append({  # Append the extracted data to the results list\n",
    "                        'Title': title,\n",
    "                        'URL': f\"https://euvsdisinfo.eu{url}\",\n",
    "                        'Summary': summary,\n",
    "                        'Response': response,\n",
    "                        'Outlet': outlet,\n",
    "                        'Date': date,\n",
    "                        'Languages': languages,\n",
    "                        'Countries': countries,\n",
    "                        'Tags': tags\n",
    "                    })\n",
    "                await article_page.close()  # Close the article page after extracting data\n",
    "\n",
    "        await browser.close()  # Close the browser after scraping\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during EUvsDISINFO search: {e}\")\n",
    "        # Optional: Take a screenshot on error for debugging\n",
    "        # if page: await page.screenshot(path='error_screenshot.png')\n",
    "\n",
    "    finally:\n",
    "        if browser:  # Ensure the browser is closed\n",
    "            await browser.close()\n",
    "            print(\"Browser closed.\")\n",
    "        if pw:\n",
    "            await pw.stop()  # Stop Playwright\n",
    "            print(\"Playwright stopped.\")\n",
    "\n",
    "    return results  # Return the results list containing the scraped data\n",
    "\n",
    "df = asyncio.run(scrape_eu_vs_disinfo())  # Run the scraping function\n",
    "if df:\n",
    "    df = pd.DataFrame(df)  # Convert the results list to a DataFrame\n",
    "    print(f\"Scraped {len(df)} results from EUvsDISINFO.\")  # Display the number of results scraped\n",
    "    df.to_csv('euvsdisinfo.csv', index=False)  # Save the DataFrame to a CSV file\n",
    "else:\n",
    "    print(\"\\nNo results scraped from EUvsDISINFO.\")  # Display a message if no results were scraped\n"
   ],
   "id": "48fe29468271f5a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Export pages separately",
   "id": "b2440c460d4a1b9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# EUvsDISINFO Scraper\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from playwright_stealth import stealth_async        # Import stealth_async for stealth mode in order to avoid detection\n",
    "\n",
    "nest_asyncio.apply()  # Apply nest_asyncio to allow nested event loops\n",
    "\n",
    "async def scrape_eu_vs_disinfo(page_num=1):\n",
    "    pw = None\n",
    "    browser = None\n",
    "    results = []  # Initialize an empty list to store results\n",
    "    try:\n",
    "        pw = await async_playwright().start()  # Start Playwright\n",
    "        browser = await pw.chromium.launch(headless=False)  # Launch Chromium browser in non-headless mode because the website detects headless browsers\n",
    "        page = await browser.new_page()  # Create a new page in the browser\n",
    "\n",
    "\n",
    "        await stealth_async(page)  # Apply stealth mode to avoid detection\n",
    "        print(f\"Navigating to page {page_num}...\")\n",
    "        url = f\"https://euvsdisinfo.eu/disinformation-cases/page/{page_num}/?disinfo_countries%5B0%5D=country_77544&_=1750022717762\"\n",
    "        await page.goto(url, timeout=120000, wait_until='domcontentloaded')  # Navigate to the page\n",
    "        print(f\"Page {page_num} loaded.\")\n",
    "        await asyncio.sleep(1) # Small delay for ethical scraping\n",
    "\n",
    "        article_cards = await page.query_selector_all('a.b-archive__database-item') # Select all article cards on the page\n",
    "        # Extract data from the first few cards\n",
    "        for i, card in enumerate(article_cards):\n",
    "            outlet = []  # Initialize an empty list for outlets\n",
    "            tags = []  # Initialize an empty list for tags\n",
    "            title = \"N/A\"  # Initialize title variable\n",
    "            summary = \"N/A\"  # Initialize summary variable\n",
    "            response = \"N/A\"  # Initialize response variable\n",
    "            date = \"N/A\"  # Initialize date variable\n",
    "            languages = []  # Initialize an empty list for languages\n",
    "            countries = []  # Initialize an empty list for countries\n",
    "\n",
    "            url = await card.get_attribute('href') # Get the URL of the article\n",
    "\n",
    "            article_page = await browser.new_page()  # Create a new page for article\n",
    "\n",
    "\n",
    "            await stealth_async(article_page)  # Apply stealth mode to avoid detection\n",
    "            await article_page.goto(f\"https://euvsdisinfo.eu{url}\", timeout=120000, wait_until='domcontentloaded')  # Navigate to the article page\n",
    "            await asyncio.sleep(1) # Small delay for ethical scraping\n",
    "\n",
    "            try:\n",
    "                await article_page.wait_for_selector('h1.b-page__title', timeout=50)  # Wait for the title to load\n",
    "            except Exception as e:\n",
    "                title_tag = await article_page.query_selector('h1.b-report__title')  # Select the title of the article\n",
    "                title = await title_tag.inner_text() if title_tag else \"N/A\"  # Get the title text\n",
    "                title = title.replace(\"DISINFO: \", \"\")  # Clean the title text\n",
    "\n",
    "                summary_tag = await article_page.query_selector('div.b-report__summary')  # Select the summary of the article\n",
    "                summary = await summary_tag.inner_text() if summary_tag else \"N/A\"  # Get the summary text\n",
    "                summary = summary.replace(\"SUMMARY\\n\\n\", \"\")  # Clean the summary text\n",
    "\n",
    "                response_tag = await article_page.query_selector('div.b-report__response')  # Select the response of the article\n",
    "                response = await response_tag.inner_text() if response_tag else \"N/A\"  # Get the response text\n",
    "                response = response.replace(\"RESPONSE\\n\\n\", \"\")  # Clean the response text\n",
    "\n",
    "                details = await article_page.query_selector_all('ul.b-report__details-list li')  # Select the details of the article\n",
    "\n",
    "                if len(details) == 2:\n",
    "                    date_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(1) span')  # Select the date of the article\n",
    "                    date = await date_tag.inner_text() if date_tag else \"N/A\"  # Get the date text\n",
    "\n",
    "                    countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(2) span')  # Select the countries of the article\n",
    "                    countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                    countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "                elif len(details) >= 3:\n",
    "                    outlet_tag = await article_page.query_selector_all('ul.b-report__details-list li:nth-child(1) a')  # Select the outlets of the article\n",
    "                    for j, outlet_item in enumerate(outlet_tag):  # Iterate through each outlet\n",
    "                        outlet_text = await outlet_item.inner_text()  # Get the outlet text\n",
    "                        if outlet_text and not outlet_text.strip().__contains__(\"archived\"):  # Clean the outlet text and append it to the outlet list\n",
    "                            outlet.append(outlet_text.strip().replace(\"\\\\n(opens in a new tab)\", \"\").replace(\"\\n(opens in a new tab)\", \"\"))\n",
    "\n",
    "                    date_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(2) span')  # Select the date of the article\n",
    "                    date = await date_tag.inner_text() if date_tag else \"N/A\"  # Get the date text\n",
    "\n",
    "                    if len(details) == 4:\n",
    "                        languages_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(3) span')  # Select the languages of the article\n",
    "                        languages = [await languages_tag.inner_text()]  # Get the languages text\n",
    "                        languages = languages[0].split(\", \") if languages else []  # Split the languages text into a list\n",
    "                        languages = list(set(languages))  # Remove duplicates\n",
    "\n",
    "                        countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(4) span')  # Select the countries of the article\n",
    "                        countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                        countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "                    elif len(details) == 3:\n",
    "                        languages = []\n",
    "\n",
    "                        countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(3) span')  # Select the countries of the article\n",
    "                        countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                        countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "\n",
    "                tags_tag = await article_page.query_selector_all('div.b-report__keywords a span')  # Select the tags of the article\n",
    "                for tag_item in tags_tag:  # Iterate through each tag\n",
    "                    tag_text = await tag_item.inner_text()  # Get the tag text\n",
    "                    if tag_text:\n",
    "                        tags.append(tag_text.strip())  # Append the tag text to the tags list\n",
    "\n",
    "                results.append({  # Append the extracted data to the results list\n",
    "                    'Title': title,\n",
    "                    'URL': f\"https://euvsdisinfo.eu{url}\",\n",
    "                    'Summary': summary,\n",
    "                    'Response': response,\n",
    "                    'Outlet': outlet,\n",
    "                    'Date': date,\n",
    "                    'Languages': languages,\n",
    "                    'Countries': countries,\n",
    "                    'Tags': tags\n",
    "                })\n",
    "            await article_page.close()  # Close the article page after extracting data\n",
    "\n",
    "        await browser.close()  # Close the browser after scraping\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during EUvsDISINFO search: {e}\")\n",
    "        # Optional: Take a screenshot on error for debugging\n",
    "        # if page: await page.screenshot(path='error_screenshot.png')\n",
    "\n",
    "    finally:\n",
    "        if browser:  # Ensure the browser is closed\n",
    "            await browser.close()\n",
    "            print(\"Browser closed.\")\n",
    "        if pw:\n",
    "            await pw.stop()  # Stop Playwright\n",
    "            print(\"Playwright stopped.\")\n",
    "\n",
    "    return results  # Return the results list containing the scraped data\n",
    "\n",
    "pages = [5, 6, 7, 10, 11, 17, 24, 25, 27, 57, 60, 65, 70, 78, 97, 211, 488]  # Pages which we had to rescrape\n",
    "\n",
    "for i in pages:\n",
    "    df = asyncio.run(scrape_eu_vs_disinfo(i))  # Run the scraping function for each page\n",
    "    if df:\n",
    "        df = pd.DataFrame(df)  # Convert the results list to a DataFrame\n",
    "        df.to_csv(f'euvsdisinfo_results_page_{i}.csv', index=False)  # Save the DataFrame to a CSV file\n",
    "    else:\n",
    "        print(f\"\\nNo results scraped from EUvsDISINFO for page {i}.\")  # Display a message if no results were scraped\n"
   ],
   "id": "5de9ebfd5fe3d008"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([])  # Initialize an empty DataFrame to store all pages\n",
    "for i in range(1, 515):\n",
    "    pg = pd.read_csv(f'euvsdisinfo_results_page_{i}.csv')  # Read the CSV file for each page\n",
    "    df = pd.concat([df, pg], ignore_index=True)  # Append the DataFrame for each page to the list\n",
    "\n",
    "df.to_csv('euvsdisinfo_results.csv', index=False)  # Save the combined DataFrame to a CSV file"
   ],
   "id": "a9133610ea6e24b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T10:40:28.881179Z",
     "start_time": "2025-06-16T10:40:12.829012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install textblob\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv('euvsdisinfo_results.csv')\n",
    "\n",
    "def get_sentiment(text):\n",
    "    if pd.isna(text):\n",
    "        return 0.0\n",
    "    return TextBlob(str(text)).sentiment.polarity\n",
    "\n",
    "df['Sentiment_Summary'] = df['Summary'].apply(get_sentiment)\n",
    "df['Sentiment_Summary_Label'] = df['Sentiment_Summary'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
    "\n",
    "df['Sentiment_Response'] = df['Response'].apply(get_sentiment)\n",
    "df['Sentiment_Response_Label'] = df['Sentiment_Response'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
    "\n",
    "df.to_csv('euvsdisinfo_results_with_sentiment.csv', index=True)\n",
    "\n",
    "print(df[['Summary', 'Sentiment_Summary', 'Sentiment_Summary_Label']].head())"
   ],
   "id": "9b7b39cfbe392b6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\marog\\desktop\\avd_project\\.venv2\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\marog\\desktop\\avd_project\\.venv2\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\marog\\desktop\\avd_project\\.venv2\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\marog\\desktop\\avd_project\\.venv2\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\marog\\desktop\\avd_project\\.venv2\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\marog\\desktop\\avd_project\\.venv2\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\marog\\desktop\\avd_project\\.venv2\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "                                             Summary  Sentiment_Summary  \\\n",
      "0  Russia proved the great strength of its econom...           0.153125   \n",
      "1  The Russian military inflicted a mass strike o...          -0.038636   \n",
      "2  The US Deep State feeds the Ukraine war. Blood...          -0.066071   \n",
      "3  Western media are concealing from their audien...           0.300000   \n",
      "4  Residents of Poland and Lithuania are fleeing ...          -0.100000   \n",
      "\n",
      "  Sentiment_Summary_Label  \n",
      "0                positive  \n",
      "1                negative  \n",
      "2                negative  \n",
      "3                positive  \n",
      "4                negative  \n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install required visualization and analysis libraries if not already installed\n",
    "!pip install wordcloud matplotlib seaborn\n",
    "!pip install networkx scikit-learn\n"
   ],
   "id": "db0387ca7c459ad5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "# Load your dataset from CSV file\n",
    "df = pd.read_csv('euvsdisinfo_results_with_sentiment.csv')\n",
    "\n",
    "# --- Find and standardize the date column ---\n",
    "col_date = [col for col in df.columns if 'date' in col.lower()][0]\n",
    "df = df.rename(columns={col_date: 'date'})\n",
    "\n",
    "# Remove possible header row accidentally duplicated in data\n",
    "df = df[df['date'].str.lower() != \"date\"]\n",
    "\n",
    "# Convert the 'date' column to pandas datetime objects for further analysis\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "\n"
   ],
   "id": "87af8022cafea2ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Plot 1: Number of articles per year ===\n",
    "df['year'] = df['date'].dt.year\n",
    "plt.figure()\n",
    "df['year'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Number of Articles per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "abcade237e2d49c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Plot 2: Number of articles per month ===\n",
    "df['month'] = df['date'].dt.to_period('M')\n",
    "plt.figure()\n",
    "df['month'].value_counts().sort_index().plot(kind='line')\n",
    "plt.title('Number of Articles per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()\n"
   ],
   "id": "df6be695f6e3084e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Plot 3: Word Cloud for Titles ===\n",
    "if 'Title' in df.columns:\n",
    "    title_text = ' '.join(df['Title'].dropna().astype(str).tolist())\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(title_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud for Titles')\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "ece1d1b4c21db057"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Plot 4: Word Cloud for Summaries ===\n",
    "if 'Summary' in df.columns:\n",
    "    desc_text = ' '.join(df['Summary'].dropna().astype(str).tolist())\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(desc_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud for Summary')\n",
    "    plt.show()\n"
   ],
   "id": "7e9eb46334c6e547"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Plot 5: Top 10 countries by occurrence ===\n",
    "if 'Countries' in df.columns:\n",
    "    filtered_countries = df[df['Countries'] != \"Countries\"]\n",
    "    country_lists = filtered_countries['Countries'].apply(ast.literal_eval)\n",
    "    all_countries = [country for countries in country_lists for country in countries]\n",
    "    country_counts = Counter(all_countries)\n",
    "    country_counts_df = pd.DataFrame(country_counts.items(), columns=['Country', 'Count']).sort_values('Count', ascending=False)\n",
    "    country_counts_df=country_counts_df.iloc[1:]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(data=country_counts_df.head(10), x='Country', y='Count')\n",
    "    plt.title('Top 10 Countries by Occurrence')\n",
    "    plt.xlabel('Country')\n",
    "    plt.ylabel('Total Occurrences')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "aab9317878cb47ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Plot 6: Top 10 outlets by number of articles ===\n",
    "if 'Outlet' in df.columns:\n",
    "    plt.figure()\n",
    "    df['Outlet'].value_counts().head(10).plot(kind='bar')\n",
    "    plt.title('Top 10 Outlets by Number of Articles')\n",
    "    plt.xlabel('Outlet')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "3bc6565e6defa4f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Plot 7: Top 10 tags by total occurrence ===\n",
    "if 'Tags' in df.columns:\n",
    "    filtered_tags = df[df['Tags'] != \"Tags\"]\n",
    "    tag_lists = filtered_tags['Tags'].apply(ast.literal_eval)\n",
    "    all_tags = [tag for tags in tag_lists for tag in tags]\n",
    "    tag_counts = Counter(all_tags)\n",
    "    tag_counts_df = pd.DataFrame(tag_counts.items(), columns=['Tag', 'Count']).sort_values('Count', ascending=False)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(data=tag_counts_df.head(10), x='Tag', y='Count')\n",
    "    plt.title('Top 10 Tags by Occurrence')\n",
    "    plt.xlabel('Tag')\n",
    "    plt.ylabel('Total Occurrences')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "dcbacd9192693dd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Plot 8: Evolution in time for a specific country (e.g., Russia) ===\n",
    "country = 'Russia'\n",
    "df['contains_russia'] = df['Countries'].apply(lambda x: country in ast.literal_eval(x) if pd.notnull(x) and x != \"Countries\" else False)\n",
    "articles_by_month = df[df['contains_russia']].groupby(df['date'].dt.to_period('M')).size()\n",
    "plt.figure(figsize=(12,5))\n",
    "articles_by_month.plot()\n",
    "plt.title(f\"Number of Articles mentioning '{country}' per Month\")\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9f356a34ae482226"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Plot 9: Average sentiment score per top 10 tags (if sentiment_score exists) ===\n",
    "if 'sentiment_score' in df.columns and 'Tags' in df.columns:\n",
    "    tag_sentiments = []\n",
    "    for tag in tag_counts_df.head(10)['Tag']:\n",
    "        mask = df['Tags'].apply(lambda x: tag in ast.literal_eval(x) if pd.notnull(x) and x != \"Tags\" else False)\n",
    "        tag_sentiments.append(df[mask]['sentiment_score'].mean())\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(x=tag_counts_df.head(10)['Tag'], y=tag_sentiments)\n",
    "    plt.title('Average Sentiment Score per Top Tag')\n",
    "    plt.xlabel('Tag')\n",
    "    plt.ylabel('Average Sentiment Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "4ff478e5962212d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:18:23.921255Z",
     "start_time": "2025-06-16T14:17:45.133164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Topic Modeling: Main topics from Summary using LDA ===\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "if 'Summary' in df.columns:\n",
    "    texts = df['Summary'].dropna().astype(str).tolist()\n",
    "    vectorizer = CountVectorizer(max_df=0.9, min_df=10, stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "    lda.fit(X)\n",
    "    for i, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic #{i}: \", [vectorizer.get_feature_names_out()[index] for index in topic.argsort()[-10:]])\n",
    "\n"
   ],
   "id": "56d2b970b832d752",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:  ['sputnik', 'western', 'elections', 'information', 'eu', 'vaccine', 'european', 'media', 'russia', 'russian']\n",
      "Topic #1:  ['poisoning', 'gas', 'world', 'western', 'chemical', 'syria', 'russian', 'west', 'navalny', 'russia']\n",
      "Topic #2:  ['international', 'kyiv', 'ukrainian', '2014', 'eu', 'sanctions', 'russian', 'crimea', 'ukraine', 'russia']\n",
      "Topic #3:  ['armed', 'regime', 'russia', 'attack', 'forces', 'military', 'kyiv', 'russian', 'ukraine', 'ukrainian']\n",
      "Topic #4:  ['countries', 'states', 'military', 'poland', 'west', 'russian', 'war', 'nato', 'ukraine', 'russia']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Plot 10: Country co-occurrence network graph (top 20) ===\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "filtered_countries = df[df['Countries'] != \"Countries\"]\n",
    "country_lists = filtered_countries['Countries'].apply(ast.literal_eval)\n",
    "edges = []\n",
    "for clist in country_lists:\n",
    "    edges += list(combinations(sorted(set(clist)), 2))\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "plt.figure(figsize=(12,12))\n",
    "top_nodes = [node for node, _ in Counter(all_countries).most_common(20)]\n",
    "nx.draw_networkx(G.subgraph(top_nodes), with_labels=True, node_size=1000, font_size=10)\n",
    "plt.title(\"Country Co-Occurrence Network (Top 20)\")\n",
    "plt.show()"
   ],
   "id": "2b30acecefbd34e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install torch transformers\n",
    "!pip install tf-keras\n"
   ],
   "id": "8dbc8a5cef4544f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:23:05.895832Z",
     "start_time": "2025-06-16T14:23:03.553271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the emotion classification pipeline\n",
    "emotion_pipe = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=None,return_all_scores=False)\n",
    "\n",
    "# Create a new column with the text length of each summary\n",
    "df['text_length'] = df['Summary'].str.len()\n",
    "\n",
    "# Mapping from emotion labels to emojis for better visualization\n",
    "emotion_to_emoji = {\n",
    "    \"anger\": \"üò†\",\n",
    "    \"disgust\": \"ü§¢\",\n",
    "    \"fear\": \"üò®\",\n",
    "    \"joy\": \"üòÑ\",\n",
    "    \"neutral\": \"üòê\",\n",
    "    \"sadness\": \"üò¢\",\n",
    "    \"surprise\": \"üò≤\",\n",
    "    \"others\": \"‚ùì\"\n",
    "}\n",
    "\n",
    "# Sort the DataFrame by text length and select the top 10 longest summaries\n",
    "top10 = df.sort_values(by='text_length', ascending=False).head(10)\n",
    "\n",
    "results = []\n",
    "# For each of the top 10 longest summaries, predict the main emotion\n",
    "for idx, row in top10.iterrows():\n",
    "    text = row['Summary']\n",
    "    # Limit the text length to 2000 characters to avoid model errors (adapt if needed)\n",
    "    text = str(text)[:2000]\n",
    "    emotion_result = emotion_pipe(text)\n",
    "    # Select the emotion with the highest score\n",
    "    main_emotion = max(emotion_result[0], key=lambda x: x['score'])\n",
    "    label = main_emotion['label']\n",
    "    # Map the emotion label to the corresponding emoji\n",
    "    emoji = emotion_to_emoji.get(label.lower(), \"‚ùì\")\n",
    "    results.append({\n",
    "        'index': idx,\n",
    "        'emotion': label,\n",
    "        'emoji': emoji,\n",
    "        'title': row['Title']  # Use 'Title' or change if your column has a different name\n",
    "    })\n",
    "\n",
    "# Create a new DataFrame with the results\n",
    "emotion_df = pd.DataFrame(results)\n",
    "\n",
    "# Print only the relevant columns: index, emotion, emoji, score, and title\n",
    "print(emotion_df[['index', 'emotion', 'emoji', 'title']])\n"
   ],
   "id": "238634355450e69",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index   emotion emoji                                              title\n",
      "0   4471   neutral     üòê     The US is hiding the truth from the MH17-trial\n",
      "1   7308   neutral     üòê  The West will support anti-Russian Ukraine unt...\n",
      "2   7449     anger     üò†  Three types of Russophobia are turning Belarus...\n",
      "3   6594   neutral     üòê  The US wishes to destabilise Belarus and provo...\n",
      "4   7276     anger     üò†  Amnesty International is an ideological tool o...\n",
      "5   5051  surprise     üò≤  EUvsDisinfo denounces disinformation from Russ...\n",
      "6    373      fear     üò®  The Greens are behind the destruction of Germa...\n",
      "7   6652   disgust     ü§¢  Western media try to minimise Soviet victory i...\n",
      "8   8484   neutral     üòê  The West has approved a new ‚ÄòBarbarossa‚Äô plan ...\n",
      "9   4575   disgust     ü§¢  Western foundations prepare the informative de...\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
