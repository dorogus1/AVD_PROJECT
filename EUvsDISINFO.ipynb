{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:23:52.789370Z",
     "start_time": "2025-06-16T08:23:13.708435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# EUvsDISINFO Scraper\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from playwright_stealth import stealth_async        # Import stealth_async for stealth mode in order to avoid detection\n",
    "\n",
    "nest_asyncio.apply()  # Apply nest_asyncio to allow nested event loops\n",
    "\n",
    "async def scrape_eu_vs_disinfo():\n",
    "    pw = None\n",
    "    browser = None\n",
    "    results = []  # Initialize an empty list to store results\n",
    "    try:\n",
    "        pw = await async_playwright().start()  # Start Playwright\n",
    "        browser = await pw.chromium.launch(headless=False)  # Launch Chromium browser in non-headless mode because the website detects headless browsers\n",
    "        page = await browser.new_page()  # Create a new page in the browser\n",
    "\n",
    "\n",
    "        await stealth_async(page)  # Apply stealth mode to avoid detection\n",
    "        url = f\"https://euvsdisinfo.eu/disinformation-cases/page/1/?disinfo_countries%5B0%5D=country_77544&_=1750026467335\"\n",
    "        await page.goto(url, timeout=120000, wait_until='domcontentloaded')  # Navigate to the page\n",
    "        await asyncio.sleep(1)  # Small delay for ethical scraping\n",
    "\n",
    "        pagination_tags = await page.query_selector('div.b-pagination a:last-child')  # Select last pagination item\n",
    "        num_pages = await pagination_tags.inner_text()  # Get the number of pages\n",
    "        print(f\"Number of pages: {num_pages}\")\n",
    "        num_pages = int(num_pages)\n",
    "\n",
    "        for page_num in range(1, num_pages + 1):  # Iterating through web pages\n",
    "            if page_num > 1:\n",
    "\n",
    "                await stealth_async(page)  # Apply stealth mode to avoid detection\n",
    "                print(f\"Navigating to page {page_num}...\")\n",
    "                url = f\"https://euvsdisinfo.eu/disinformation-cases/page/{page_num}/?disinfo_countries%5B0%5D=country_77544&_=1750026467335\"\n",
    "                await page.goto(url, timeout=120000, wait_until='domcontentloaded')  # Navigate to the page\n",
    "                print(f\"Page {page_num} loaded.\")\n",
    "                await asyncio.sleep(1)  # Small delay for ethical scraping\n",
    "\n",
    "            article_cards = await page.query_selector_all('a.b-archive__database-item') # Select all article cards on the page\n",
    "\n",
    "            # Extract data from the first few cards\n",
    "            for i, card in enumerate(article_cards):\n",
    "                outlet = []  # Initialize an empty list for outlets\n",
    "                tags = []  # Initialize an empty list for tags\n",
    "                title = \"N/A\"  # Initialize title as \"N/A\"\n",
    "                summary = \"N/A\"  # Initialize summary as \"N/A\"\n",
    "                response = \"N/A\"  # Initialize response as \"N/A\"\n",
    "                date = \"N/A\"  # Initialize date as \"N/A\"\n",
    "                languages = []  # Initialize an empty list for languages\n",
    "                countries = []  # Initialize an empty list for countries\n",
    "\n",
    "                url = await card.get_attribute('href') # Get the URL of the article\n",
    "\n",
    "                article_page = await browser.new_page()  # Create a new page for article\n",
    "\n",
    "\n",
    "                await stealth_async(article_page)  # Apply stealth mode to avoid detection\n",
    "                await article_page.goto(f\"https://euvsdisinfo.eu{url}\", timeout=120000, wait_until='domcontentloaded')  # Navigate to the article page\n",
    "                await asyncio.sleep(1) # Small delay for ethical scraping\n",
    "\n",
    "                try:\n",
    "                    await article_page.wait_for_selector('h1.b-page__title', timeout=100)  # Wait for the title to load\n",
    "                except Exception as e:\n",
    "                    title_tag = await article_page.query_selector('h1.b-report__title')  # Select the title of the article\n",
    "                    title = await title_tag.inner_text() if title_tag else \"N/A\"  # Get the title text\n",
    "                    title = title.replace(\"DISINFO: \", \"\")  # Clean the title text\n",
    "\n",
    "                    summary_tag = await article_page.query_selector('div.b-report__summary')  # Select the summary of the article\n",
    "                    summary = await summary_tag.inner_text() if summary_tag else \"N/A\"  # Get the summary text\n",
    "                    summary = summary.replace(\"SUMMARY\\n\\n\", \"\")  # Clean the summary text\n",
    "\n",
    "                    response_tag = await article_page.query_selector('div.b-report__response')  # Select the response of the article\n",
    "                    response = await response_tag.inner_text() if response_tag else \"N/A\"  # Get the response text\n",
    "                    response = response.replace(\"RESPONSE\\n\\n\", \"\")  # Clean the response text\n",
    "\n",
    "                    details = await article_page.query_selector_all('ul.b-report__details-list li')  # Select the details of the article\n",
    "\n",
    "                    if len(details) == 2:\n",
    "                        date_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(1) span')  # Select the date of the article\n",
    "                        date = await date_tag.inner_text() if date_tag else \"N/A\"  # Get the date text\n",
    "\n",
    "                        countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(2) span')  # Select the countries of the article\n",
    "                        countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                        countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "                    elif len(details) >= 3:\n",
    "                        outlet_tag = await article_page.query_selector_all('ul.b-report__details-list li:nth-child(1) a')  # Select the outlets of the article\n",
    "                        for j, outlet_item in enumerate(outlet_tag):  # Iterate through each outlet\n",
    "                            outlet_text = await outlet_item.inner_text()  # Get the outlet text\n",
    "                            if outlet_text and not outlet_text.strip().__contains__(\"archived\"):  # Clean the outlet text and append it to the outlet list\n",
    "                                outlet.append(outlet_text.strip().replace(\"\\\\n(opens in a new tab)\", \"\").replace(\"\\n(opens in a new tab)\", \"\"))\n",
    "\n",
    "                        date_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(2) span')  # Select the date of the article\n",
    "                        date = await date_tag.inner_text() if date_tag else \"N/A\"  # Get the date text\n",
    "\n",
    "                        if len(details) == 4:\n",
    "                            languages_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(3) span')  # Select the languages of the article\n",
    "                            languages = [await languages_tag.inner_text()]  # Get the languages text\n",
    "                            languages = languages[0].split(\", \") if languages else []  # Split the languages text into a list\n",
    "                            languages = list(set(languages))  # Remove duplicates\n",
    "\n",
    "                            countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(4) span')  # Select the countries of the article\n",
    "                            countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                            countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "                        elif len(details) == 3:\n",
    "                            languages = []\n",
    "\n",
    "                            countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(3) span')  # Select the countries of the article\n",
    "                            countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                            countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "\n",
    "                    tags_tag = await article_page.query_selector_all('div.b-report__keywords a span')  # Select the tags of the article\n",
    "                    for tag_item in tags_tag:  # Iterate through each tag\n",
    "                        tag_text = await tag_item.inner_text()  # Get the tag text\n",
    "                        if tag_text:\n",
    "                            tags.append(tag_text.strip())  # Append the tag text to the tags list\n",
    "\n",
    "                    results.append({  # Append the extracted data to the results list\n",
    "                        'Title': title,\n",
    "                        'URL': f\"https://euvsdisinfo.eu{url}\",\n",
    "                        'Summary': summary,\n",
    "                        'Response': response,\n",
    "                        'Outlet': outlet,\n",
    "                        'Date': date,\n",
    "                        'Languages': languages,\n",
    "                        'Countries': countries,\n",
    "                        'Tags': tags\n",
    "                    })\n",
    "                await article_page.close()  # Close the article page after extracting data\n",
    "\n",
    "        await browser.close()  # Close the browser after scraping\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during EUvsDISINFO search: {e}\")\n",
    "        # Optional: Take a screenshot on error for debugging\n",
    "        # if page: await page.screenshot(path='error_screenshot.png')\n",
    "\n",
    "    finally:\n",
    "        if browser:  # Ensure the browser is closed\n",
    "            await browser.close()\n",
    "            print(\"Browser closed.\")\n",
    "        if pw:\n",
    "            await pw.stop()  # Stop Playwright\n",
    "            print(\"Playwright stopped.\")\n",
    "\n",
    "    return results  # Return the results list containing the scraped data\n",
    "\n",
    "df = asyncio.run(scrape_eu_vs_disinfo())  # Run the scraping function\n",
    "if df:\n",
    "    df = pd.DataFrame(df)  # Convert the results list to a DataFrame\n",
    "    print(f\"Scraped {len(df)} results from EUvsDISINFO.\")  # Display the number of results scraped\n",
    "    df.to_csv('euvsdisinfo.csv', index=False)  # Save the DataFrame to a CSV file\n",
    "else:\n",
    "    print(\"\\nNo results scraped from EUvsDISINFO.\")  # Display a message if no results were scraped\n"
   ],
   "id": "a7e3d4b4115967a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 514\n",
      "Navigating to page 455...\n",
      "Page 455 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Scraped 18 results from EUvsDISINFO.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Export pages separately",
   "id": "8f8bb5a931367f0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T07:27:54.089562Z",
     "start_time": "2025-06-16T07:10:41.191800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# EUvsDISINFO Scraper\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from playwright_stealth import stealth_async        # Import stealth_async for stealth mode in order to avoid detection\n",
    "\n",
    "nest_asyncio.apply()  # Apply nest_asyncio to allow nested event loops\n",
    "\n",
    "async def scrape_eu_vs_disinfo(page_num=1):\n",
    "    pw = None\n",
    "    browser = None\n",
    "    results = []  # Initialize an empty list to store results\n",
    "    try:\n",
    "        pw = await async_playwright().start()  # Start Playwright\n",
    "        browser = await pw.chromium.launch(headless=False)  # Launch Chromium browser in non-headless mode because the website detects headless browsers\n",
    "        page = await browser.new_page()  # Create a new page in the browser\n",
    "\n",
    "\n",
    "        await stealth_async(page)  # Apply stealth mode to avoid detection\n",
    "        print(f\"Navigating to page {page_num}...\")\n",
    "        url = f\"https://euvsdisinfo.eu/disinformation-cases/page/{page_num}/?disinfo_countries%5B0%5D=country_77544&_=1750022717762\"\n",
    "        await page.goto(url, timeout=120000, wait_until='domcontentloaded')  # Navigate to the page\n",
    "        print(f\"Page {page_num} loaded.\")\n",
    "        await asyncio.sleep(1) # Small delay for ethical scraping\n",
    "\n",
    "        article_cards = await page.query_selector_all('a.b-archive__database-item') # Select all article cards on the page\n",
    "        # Extract data from the first few cards\n",
    "        for i, card in enumerate(article_cards):\n",
    "            outlet = []  # Initialize an empty list for outlets\n",
    "            tags = []  # Initialize an empty list for tags\n",
    "            title = \"N/A\"  # Initialize title variable\n",
    "            summary = \"N/A\"  # Initialize summary variable\n",
    "            response = \"N/A\"  # Initialize response variable\n",
    "            date = \"N/A\"  # Initialize date variable\n",
    "            languages = []  # Initialize an empty list for languages\n",
    "            countries = []  # Initialize an empty list for countries\n",
    "\n",
    "            url = await card.get_attribute('href') # Get the URL of the article\n",
    "\n",
    "            article_page = await browser.new_page()  # Create a new page for article\n",
    "\n",
    "\n",
    "            await stealth_async(article_page)  # Apply stealth mode to avoid detection\n",
    "            await article_page.goto(f\"https://euvsdisinfo.eu{url}\", timeout=120000, wait_until='domcontentloaded')  # Navigate to the article page\n",
    "            await asyncio.sleep(1) # Small delay for ethical scraping\n",
    "\n",
    "            try:\n",
    "                await article_page.wait_for_selector('h1.b-page__title', timeout=50)  # Wait for the title to load\n",
    "            except Exception as e:\n",
    "                title_tag = await article_page.query_selector('h1.b-report__title')  # Select the title of the article\n",
    "                title = await title_tag.inner_text() if title_tag else \"N/A\"  # Get the title text\n",
    "                title = title.replace(\"DISINFO: \", \"\")  # Clean the title text\n",
    "\n",
    "                summary_tag = await article_page.query_selector('div.b-report__summary')  # Select the summary of the article\n",
    "                summary = await summary_tag.inner_text() if summary_tag else \"N/A\"  # Get the summary text\n",
    "                summary = summary.replace(\"SUMMARY\\n\\n\", \"\")  # Clean the summary text\n",
    "\n",
    "                response_tag = await article_page.query_selector('div.b-report__response')  # Select the response of the article\n",
    "                response = await response_tag.inner_text() if response_tag else \"N/A\"  # Get the response text\n",
    "                response = response.replace(\"RESPONSE\\n\\n\", \"\")  # Clean the response text\n",
    "\n",
    "                details = await article_page.query_selector_all('ul.b-report__details-list li')  # Select the details of the article\n",
    "\n",
    "                if len(details) == 2:\n",
    "                    date_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(1) span')  # Select the date of the article\n",
    "                    date = await date_tag.inner_text() if date_tag else \"N/A\"  # Get the date text\n",
    "\n",
    "                    countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(2) span')  # Select the countries of the article\n",
    "                    countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                    countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "                elif len(details) >= 3:\n",
    "                    outlet_tag = await article_page.query_selector_all('ul.b-report__details-list li:nth-child(1) a')  # Select the outlets of the article\n",
    "                    for j, outlet_item in enumerate(outlet_tag):  # Iterate through each outlet\n",
    "                        outlet_text = await outlet_item.inner_text()  # Get the outlet text\n",
    "                        if outlet_text and not outlet_text.strip().__contains__(\"archived\"):  # Clean the outlet text and append it to the outlet list\n",
    "                            outlet.append(outlet_text.strip().replace(\"\\\\n(opens in a new tab)\", \"\").replace(\"\\n(opens in a new tab)\", \"\"))\n",
    "\n",
    "                    date_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(2) span')  # Select the date of the article\n",
    "                    date = await date_tag.inner_text() if date_tag else \"N/A\"  # Get the date text\n",
    "\n",
    "                    if len(details) == 4:\n",
    "                        languages_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(3) span')  # Select the languages of the article\n",
    "                        languages = [await languages_tag.inner_text()]  # Get the languages text\n",
    "                        languages = languages[0].split(\", \") if languages else []  # Split the languages text into a list\n",
    "                        languages = list(set(languages))  # Remove duplicates\n",
    "\n",
    "                        countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(4) span')  # Select the countries of the article\n",
    "                        countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                        countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "                    elif len(details) == 3:\n",
    "                        languages = []\n",
    "\n",
    "                        countries_tag = await article_page.query_selector('ul.b-report__details-list li:nth-child(3) span')  # Select the countries of the article\n",
    "                        countries = [await countries_tag.inner_text()]  # Get the countries text\n",
    "                        countries = countries[0].split(\", \") if countries else []  # Split the countries text into a list\n",
    "\n",
    "                tags_tag = await article_page.query_selector_all('div.b-report__keywords a span')  # Select the tags of the article\n",
    "                for tag_item in tags_tag:  # Iterate through each tag\n",
    "                    tag_text = await tag_item.inner_text()  # Get the tag text\n",
    "                    if tag_text:\n",
    "                        tags.append(tag_text.strip())  # Append the tag text to the tags list\n",
    "\n",
    "                results.append({  # Append the extracted data to the results list\n",
    "                    'Title': title,\n",
    "                    'URL': f\"https://euvsdisinfo.eu{url}\",\n",
    "                    'Summary': summary,\n",
    "                    'Response': response,\n",
    "                    'Outlet': outlet,\n",
    "                    'Date': date,\n",
    "                    'Languages': languages,\n",
    "                    'Countries': countries,\n",
    "                    'Tags': tags\n",
    "                })\n",
    "            await article_page.close()  # Close the article page after extracting data\n",
    "\n",
    "        await browser.close()  # Close the browser after scraping\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during EUvsDISINFO search: {e}\")\n",
    "        # Optional: Take a screenshot on error for debugging\n",
    "        # if page: await page.screenshot(path='error_screenshot.png')\n",
    "\n",
    "    finally:\n",
    "        if browser:  # Ensure the browser is closed\n",
    "            await browser.close()\n",
    "            print(\"Browser closed.\")\n",
    "        if pw:\n",
    "            await pw.stop()  # Stop Playwright\n",
    "            print(\"Playwright stopped.\")\n",
    "\n",
    "    return results  # Return the results list containing the scraped data\n",
    "\n",
    "pages = [5, 6, 7, 10, 11, 17, 24, 25, 27, 57, 60, 65, 70, 78, 97, 211, 488]  # Pages which we had to rescrape\n",
    "\n",
    "for i in pages:\n",
    "    df = asyncio.run(scrape_eu_vs_disinfo(i))  # Run the scraping function for each page\n",
    "    if df:\n",
    "        df = pd.DataFrame(df)  # Convert the results list to a DataFrame\n",
    "        df.to_csv(f'euvsdisinfo_results_page_{i}.csv', index=False)  # Save the DataFrame to a CSV file\n",
    "    else:\n",
    "        print(f\"\\nNo results scraped from EUvsDISINFO for page {i}.\")  # Display a message if no results were scraped\n"
   ],
   "id": "8ce38f7469381a90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to page 5...\n",
      "Page 5 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 6...\n",
      "Page 6 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 7...\n",
      "Page 7 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 10...\n",
      "Page 10 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 11...\n",
      "Page 11 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 17...\n",
      "Page 17 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 24...\n",
      "Page 24 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 25...\n",
      "Page 25 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 27...\n",
      "Page 27 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 57...\n",
      "Page 57 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 60...\n",
      "Page 60 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 65...\n",
      "Page 65 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 70...\n",
      "Page 70 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 78...\n",
      "Page 78 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 97...\n",
      "Page 97 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 211...\n",
      "Page 211 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n",
      "Navigating to page 488...\n",
      "Page 488 loaded.\n",
      "Browser closed.\n",
      "Playwright stopped.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T08:04:27.767798Z",
     "start_time": "2025-06-16T08:04:24.722905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([])  # Initialize an empty DataFrame to store all pages\n",
    "for i in range(1, 515):\n",
    "    pg = pd.read_csv(f'euvsdisinfo_results_page_{i}.csv')  # Read the CSV file for each page\n",
    "    df = pd.concat([df, pg], ignore_index=True)  # Append the DataFrame for each page to the list\n",
    "\n",
    "df.to_csv('euvsdisinfo_results.csv', index=False)  # Save the combined DataFrame to a CSV file"
   ],
   "id": "cd5416e5298d4edf",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
