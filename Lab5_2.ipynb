{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br_xQPTTOfLE"
   },
   "source": [
    "# Lab 5: Web Scraping, APIs, and Topic Modeling"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T06:52:19.178183Z",
     "start_time": "2025-05-08T06:52:19.169302Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T06:52:19.214778Z",
     "start_time": "2025-05-08T06:52:19.207179Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3N6LDMYg8Ao"
   },
   "source": [
    "\n",
    "## Part 3: Accessing Structured Data with APIs - The Reddit Case Study (PRAW)\n",
    "\n",
    "Sometimes, websites *want* you to access their data programmatically and provide an **API** (Application Programming Interface). This is like a formal contract: you make requests in a specific format, and the website returns structured data (often in JSON format), without you needing to parse messy HTML. It's generally more reliable and efficient than scraping when available.\n",
    "\n",
    "**Reddit** has a popular API, and the **PRAW** (Python Reddit API Wrapper) library makes using it incredibly easy.\n",
    "\n",
    "**Why use PRAW instead of scraping Reddit?**\n",
    "*   **Reliability:** Reddit's HTML structure can change often, breaking scrapers. The API structure is much more stable.\n",
    "*   **Efficiency:** APIs usually return exactly the data you need, often faster than downloading and parsing large HTML pages.\n",
    "*   **Rate Limits & Rules:** PRAW helps manage API usage limits automatically, preventing you from getting blocked (as easily). Scraping might violate Reddit's Terms of Service if done improperly.\n",
    "*   **Functionality:** PRAW lets you not only *read* data but also interact (comment, upvote, post, etc.) if you authenticate properly.\n",
    "\n",
    "Let's set up PRAW! You'll need Reddit API credentials.\n",
    "\n",
    "**Steps to get Reddit API Credentials:**\n",
    "1.  Log in to your Reddit account.\n",
    "2.  Go to: `https://www.reddit.com/prefs/apps`\n",
    "3.  Scroll down and click \"are you a developer? create an app...\"\n",
    "4.  Fill in the form:\n",
    "    *   **Name:** Give your script a unique name (e.g., `TimișoaraDataMiningLab`).\n",
    "    *   **Type:** Select `script`.\n",
    "    *   **Description:** (Optional)\n",
    "    *   **About URL:** (Optional)\n",
    "    *   **Redirect URI:** For script apps, `http://localhost:8080` is commonly used (it won't actually be used for simple data fetching).\n",
    "5.  Click \"create app\".\n",
    "6.  You'll see your app listed. Note down the **client ID** (under the app name) and the **client secret**.\n",
    "7.  Also, create a **user agent** string. It should be unique and descriptive, including your Reddit username. Format: `<platform>:<app_id>:<version> (by /u/<your_reddit_username>)` e.g., `python:TimișoaraDataMiningLab:v1.0 (by /u/YourUsernameHere)`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WG69qXGAOrzK",
    "outputId": "a734cd4f-93d8-4785-b6a3-ca8b4e8eb674",
    "ExecuteTime": {
     "end_time": "2025-05-08T06:52:27.457356Z",
     "start_time": "2025-05-08T06:52:19.290917Z"
    }
   },
   "source": [
    "# @title Installing and Setting up PRAW\n",
    "!pip install praw pandas --quiet\n",
    "import praw\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore PRAW's specific warning about async deprecation if not using async features heavily\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module='praw')\n",
    "\n",
    "# --- !!! IMPORTANT !!! ---\n",
    "# Replace placeholders with your actual Reddit API Credentials\n",
    "# It's best practice NOT to hardcode credentials directly in scripts.\n",
    "# Consider using environment variables or a config file in real projects.\n",
    "CLIENT_ID = '3xKMNU3l7Uu_YNsUFUjB-g' # Paste your client ID\n",
    "CLIENT_SECRET = 'wJQ1FhdcwNPHFbMQNabUZOERLZreVg' # Paste your client secret\n",
    "USER_AGENT = 'python:TimișoaraDataMiningLab' # Customize with your app name and username\n",
    "\n",
    "# For demonstration purposes ONLY, here are dummy credentials (replace with your own!)\n",
    "# DO NOT USE THESE FOR ACTUAL SCRAPING - THEY LIKELY WON'T WORK OR ARE RATE-LIMITED\n",
    "DEMO_CLIENT_ID = 'sIXZTihLNiKiHw'\n",
    "DEMO_CLIENT_SECRET = 'EjfAsmz5z8mDbZohe4UPYTPIZsYmOQ'\n",
    "DEMO_USER_AGENT = 'TestAgentPraw:v1 (by /u/TemporaryUser)'\n",
    "\n",
    "try:\n",
    "    # Create a Reddit instance (read-only is sufficient for fetching posts)\n",
    "    # Set check_for_async=False if you are running in a synchronous environment like default Colab/Jupyter cells\n",
    "    # reddit = praw.Reddit(\n",
    "    #     client_id=CLIENT_ID,\n",
    "    #     client_secret=CLIENT_SECRET,\n",
    "    #     user_agent=USER_AGENT,\n",
    "    #     check_for_async=False # Important for standard notebooks\n",
    "    # )\n",
    "\n",
    "    # For the sake of example, below you have a test user created for you. Please try to switch to your actual user asap\n",
    "    reddit = praw.Reddit(client_id='sIXZTihLNiKiHw',\n",
    "                     client_secret='EjfAsmz5z8mDbZohe4UPYTPIZsYmOQ',\n",
    "                     user_agent='Test1', check_for_async=False)\n",
    "\n",
    "    # Verify connection by checking the read-only status\n",
    "    print(f\"PRAW instance created. Read-only: {reddit.read_only}\")\n",
    "    # You can optionally fetch something small to fully verify, like the site name\n",
    "    print(f\"Connected to Reddit site: {reddit.config.reddit_url}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating PRAW instance: {e}\")\n",
    "    print(\"Please ensure you have replaced the placeholder credentials with your actual ones.\")\n",
    "    print(\"Using dummy credentials for demonstration purposes (might not work).\")\n",
    "    # Fallback to dummy credentials for code execution (will likely fail on real requests)\n",
    "    reddit = praw.Reddit(client_id=DEMO_CLIENT_ID,\n",
    "                        client_secret=DEMO_CLIENT_SECRET,\n",
    "                        user_agent=DEMO_USER_AGENT,\n",
    "                        check_for_async=False)\n",
    "    print(f\"PRAW instance created with dummy credentials. Read-only: {reddit.read_only}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRAW instance created. Read-only: True\n",
      "Connected to Reddit site: https://www.reddit.com\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T06:52:27.508298Z",
     "start_time": "2025-05-08T06:52:27.500267Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7eajqH-eOym5",
    "outputId": "d3acca3e-7e29-47c4-b5e5-953911396f7f",
    "ExecuteTime": {
     "end_time": "2025-05-08T06:52:28.705064Z",
     "start_time": "2025-05-08T06:52:27.553954Z"
    }
   },
   "source": [
    "# @title Fetching Hot Posts from a Subreddit\n",
    "\n",
    "# Let's explore the r/Romania subreddit\n",
    "target_subreddit = 'Romania'\n",
    "post_limit = 10\n",
    "\n",
    "print(f\"\\nFetching the top {post_limit} 'hot' posts from r/{target_subreddit}...\")\n",
    "\n",
    "try:\n",
    "    if reddit.read_only is False and CLIENT_ID == 'YOUR_CLIENT_ID_HERE':\n",
    "         print(\"\\nWARNING: Using placeholder credentials. Fetching might fail.\")\n",
    "\n",
    "    # Get the subreddit instance\n",
    "    subreddit = reddit.subreddit(target_subreddit)\n",
    "\n",
    "    # Fetch hot posts\n",
    "    hot_posts = subreddit.hot(limit=post_limit)\n",
    "\n",
    "    # Iterate and print post titles and scores\n",
    "    for post in hot_posts:\n",
    "        # post.title is the title, post.score is the upvote count\n",
    "        # post.stickied indicates if it's a pinned mod post\n",
    "        print(f\"- {'[STICKIED] ' if post.stickied else ''}{post.title} (Score: {post.score})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError fetching posts from r/{target_subreddit}: {e}\")\n",
    "    if \"401\" in str(e):\n",
    "        print(\"This might be due to invalid API credentials.\")\n",
    "    elif \"404\" in str(e):\n",
    "         print(f\"Subreddit r/{target_subreddit} might not exist or is private.\")\n",
    "    else:\n",
    "        print(\"Check your internet connection or API credentials.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching the top 10 'hot' posts from r/Romania...\n",
      "- [STICKIED] Forum Liber - Întrebați și discutați cu /r/Romania Orice - 05.05.2025 (Score: 1)\n",
      "- [STICKIED] MEGATHREAD Alegeri prezidențiale turul I (4 Mai 2025) (Score: 171)\n",
      "- Marketing d-ala blanao (Score: 343)\n",
      "- It's gone =)) (Score: 1229)\n",
      "- Marketing (Score: 150)\n",
      "- Demagogie vs. matematică (Score: 128)\n",
      "- De doi ani în cazul fetei violate la școala de vară AUR nu se face nimic. Avocată: E un partid cu o solidă componentă militarizată, profund înfiptă în sistem - spotmedia.ro (Score: 158)\n",
      "- Ce interesat e Dughin de Simion, il pune direct la pinned (Score: 112)\n",
      "- Simion e ăla care coboară împreună cu portofelul tău la prima. Nu și cu țara. (Score: 617)\n",
      "- Portrete virtuale pe 500 de votanti Simion/Georgescu - Text preluat de pe Facebook (Score: 109)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "hM07DuGAPKHg",
    "outputId": "1777f4ac-6e86-4b7a-a5a1-700044327085",
    "ExecuteTime": {
     "end_time": "2025-05-08T06:52:33.805405Z",
     "start_time": "2025-05-08T06:52:29.520354Z"
    }
   },
   "source": [
    "# @title Fetching Posts and Creating a DataFrame\n",
    "from datetime import datetime\n",
    "\n",
    "# Let's get more data and put it into a structured format\n",
    "target_subreddit = 'programare' # Maybe a Romanian programming subreddit?\n",
    "post_limit = 15 # Get a few more posts\n",
    "fetch_mode = 'top' # Let's get 'top' posts (options: 'hot', 'new', 'top', 'controversial')\n",
    "time_filter = 'month' # For 'top' and 'controversial', specify time: 'all', 'year', 'month', 'week', 'day', 'hour'\n",
    "\n",
    "print(f\"\\nFetching the {fetch_mode} {post_limit} posts from r/{target_subreddit} (time filter: {time_filter})...\")\n",
    "\n",
    "posts_data = []\n",
    "\n",
    "try:\n",
    "    if reddit.read_only is False and CLIENT_ID == 'YOUR_CLIENT_ID_HERE':\n",
    "         print(\"\\nWARNING: Using placeholder credentials. Fetching might fail.\")\n",
    "\n",
    "    subreddit = reddit.subreddit(target_subreddit)\n",
    "\n",
    "    # Use the specified fetch mode and time filter\n",
    "    if fetch_mode == 'hot':\n",
    "        post_iterator = subreddit.hot(limit=post_limit)\n",
    "    elif fetch_mode == 'new':\n",
    "        post_iterator = subreddit.new(limit=post_limit)\n",
    "    elif fetch_mode == 'top':\n",
    "        post_iterator = subreddit.top(time_filter=time_filter, limit=post_limit)\n",
    "    elif fetch_mode == 'controversial':\n",
    "         post_iterator = subreddit.controversial(time_filter=time_filter, limit=post_limit)\n",
    "    else:\n",
    "        print(f\"Invalid fetch_mode: {fetch_mode}. Defaulting to 'hot'.\")\n",
    "        post_iterator = subreddit.hot(limit=post_limit)\n",
    "\n",
    "\n",
    "    for post in post_iterator:\n",
    "        # Convert UTC timestamp to readable datetime\n",
    "        post_time = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "\n",
    "        posts_data.append([\n",
    "            post.title,\n",
    "            post.score,\n",
    "            post.id,\n",
    "            post.subreddit.display_name, # Get subreddit name\n",
    "            post.url, # URL of the post\n",
    "            post.num_comments,\n",
    "            post.selftext, # The body text of the post (if it's a text post)\n",
    "            post_time # Use the converted time\n",
    "        ])\n",
    "\n",
    "    # Create DataFrame\n",
    "    posts_df = pd.DataFrame(posts_data, columns=[\n",
    "        'title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created_time'\n",
    "    ])\n",
    "\n",
    "    print(f\"\\nSuccessfully fetched {len(posts_df)} posts.\")\n",
    "    print(\"--- Sample Posts DataFrame ---\")\n",
    "    display(posts_df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError fetching posts: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching the top 15 posts from r/programare (time filter: month)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 34\u001B[0m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid fetch_mode: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfetch_mode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Defaulting to \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhot\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     31\u001B[0m     post_iterator \u001B[38;5;241m=\u001B[39m subreddit\u001B[38;5;241m.\u001B[39mhot(limit\u001B[38;5;241m=\u001B[39mpost_limit)\n\u001B[1;32m---> 34\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m post \u001B[38;5;129;01min\u001B[39;00m post_iterator:\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;66;03m# Convert UTC timestamp to readable datetime\u001B[39;00m\n\u001B[0;32m     36\u001B[0m     post_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mutcfromtimestamp(post\u001B[38;5;241m.\u001B[39mcreated_utc)\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mS UTC\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     38\u001B[0m     posts_data\u001B[38;5;241m.\u001B[39mappend([\n\u001B[0;32m     39\u001B[0m         post\u001B[38;5;241m.\u001B[39mtitle,\n\u001B[0;32m     40\u001B[0m         post\u001B[38;5;241m.\u001B[39mscore,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     46\u001B[0m         post_time \u001B[38;5;66;03m# Use the converted time\u001B[39;00m\n\u001B[0;32m     47\u001B[0m     ])\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\praw\\models\\listing\\generator.py:66\u001B[0m, in \u001B[0;36mListingGenerator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_list_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing):\n\u001B[1;32m---> 66\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_batch()\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_list_index \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39myielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\praw\\models\\listing\\generator.py:90\u001B[0m, in \u001B[0;36mListingGenerator._next_batch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exhausted:\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n\u001B[1;32m---> 90\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reddit\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murl, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extract_sublist(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing)\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_list_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\praw\\util\\deprecate_args.py:46\u001B[0m, in \u001B[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     39\u001B[0m     arg_string \u001B[38;5;241m=\u001B[39m _generate_arg_string(_old_args[: \u001B[38;5;28mlen\u001B[39m(args)])\n\u001B[0;32m     40\u001B[0m     warn(\n\u001B[0;32m     41\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPositional arguments for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m will no longer be\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     42\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m supported in PRAW 8.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCall this function with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marg_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[0;32m     44\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m     45\u001B[0m     )\n\u001B[1;32m---> 46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(_old_args, args)), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\praw\\reddit.py:731\u001B[0m, in \u001B[0;36mReddit.get\u001B[1;34m(self, path, params)\u001B[0m\n\u001B[0;32m    718\u001B[0m \u001B[38;5;129m@_deprecate_args\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    719\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\n\u001B[0;32m    720\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    723\u001B[0m     params: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    724\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    725\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001B[39;00m\n\u001B[0;32m    726\u001B[0m \n\u001B[0;32m    727\u001B[0m \u001B[38;5;124;03m    :param path: The path to fetch.\u001B[39;00m\n\u001B[0;32m    728\u001B[0m \u001B[38;5;124;03m    :param params: The query parameters to add to the request (default: ``None``).\u001B[39;00m\n\u001B[0;32m    729\u001B[0m \n\u001B[0;32m    730\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 731\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_objectify_request(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGET\u001B[39m\u001B[38;5;124m\"\u001B[39m, params\u001B[38;5;241m=\u001B[39mparams, path\u001B[38;5;241m=\u001B[39mpath)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\praw\\reddit.py:514\u001B[0m, in \u001B[0;36mReddit._objectify_request\u001B[1;34m(self, data, files, json, method, params, path)\u001B[0m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_objectify_request\u001B[39m(\n\u001B[0;32m    489\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    490\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    496\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    497\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    498\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Run a request through the ``Objector``.\u001B[39;00m\n\u001B[0;32m    499\u001B[0m \n\u001B[0;32m    500\u001B[0m \u001B[38;5;124;03m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m \n\u001B[0;32m    512\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_objector\u001B[38;5;241m.\u001B[39mobjectify(\n\u001B[1;32m--> 514\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(\n\u001B[0;32m    515\u001B[0m             data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[0;32m    516\u001B[0m             files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[0;32m    517\u001B[0m             json\u001B[38;5;241m=\u001B[39mjson,\n\u001B[0;32m    518\u001B[0m             method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[0;32m    519\u001B[0m             params\u001B[38;5;241m=\u001B[39mparams,\n\u001B[0;32m    520\u001B[0m             path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m    521\u001B[0m         )\n\u001B[0;32m    522\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\praw\\util\\deprecate_args.py:46\u001B[0m, in \u001B[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     39\u001B[0m     arg_string \u001B[38;5;241m=\u001B[39m _generate_arg_string(_old_args[: \u001B[38;5;28mlen\u001B[39m(args)])\n\u001B[0;32m     40\u001B[0m     warn(\n\u001B[0;32m     41\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPositional arguments for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m will no longer be\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     42\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m supported in PRAW 8.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCall this function with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marg_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[0;32m     44\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m     45\u001B[0m     )\n\u001B[1;32m---> 46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(_old_args, args)), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\praw\\reddit.py:963\u001B[0m, in \u001B[0;36mReddit.request\u001B[1;34m(self, data, files, json, method, params, path)\u001B[0m\n\u001B[0;32m    961\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ClientException(msg)\n\u001B[0;32m    962\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 963\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_core\u001B[38;5;241m.\u001B[39mrequest(\n\u001B[0;32m    964\u001B[0m         data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[0;32m    965\u001B[0m         files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[0;32m    966\u001B[0m         json\u001B[38;5;241m=\u001B[39mjson,\n\u001B[0;32m    967\u001B[0m         method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[0;32m    968\u001B[0m         params\u001B[38;5;241m=\u001B[39mparams,\n\u001B[0;32m    969\u001B[0m         path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m    970\u001B[0m     )\n\u001B[0;32m    971\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m BadRequest \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[0;32m    972\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\prawcore\\sessions.py:328\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, path, data, files, json, params, timeout)\u001B[0m\n\u001B[0;32m    326\u001B[0m     json[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapi_type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    327\u001B[0m url \u001B[38;5;241m=\u001B[39m urljoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_requestor\u001B[38;5;241m.\u001B[39moauth_url, path)\n\u001B[1;32m--> 328\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request_with_retries(\n\u001B[0;32m    329\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[0;32m    330\u001B[0m     files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[0;32m    331\u001B[0m     json\u001B[38;5;241m=\u001B[39mjson,\n\u001B[0;32m    332\u001B[0m     method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[0;32m    333\u001B[0m     params\u001B[38;5;241m=\u001B[39mparams,\n\u001B[0;32m    334\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    335\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m    336\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\prawcore\\sessions.py:234\u001B[0m, in \u001B[0;36mSession._request_with_retries\u001B[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001B[0m\n\u001B[0;32m    232\u001B[0m retry_strategy_state\u001B[38;5;241m.\u001B[39msleep()\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_request(data, method, params, url)\n\u001B[1;32m--> 234\u001B[0m response, saved_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[0;32m    235\u001B[0m     data,\n\u001B[0;32m    236\u001B[0m     files,\n\u001B[0;32m    237\u001B[0m     json,\n\u001B[0;32m    238\u001B[0m     method,\n\u001B[0;32m    239\u001B[0m     params,\n\u001B[0;32m    240\u001B[0m     retry_strategy_state,\n\u001B[0;32m    241\u001B[0m     timeout,\n\u001B[0;32m    242\u001B[0m     url,\n\u001B[0;32m    243\u001B[0m )\n\u001B[0;32m    245\u001B[0m do_retry \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m codes[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munauthorized\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\prawcore\\sessions.py:186\u001B[0m, in \u001B[0;36mSession._make_request\u001B[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_make_request\u001B[39m(\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    176\u001B[0m     data: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mtuple\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    183\u001B[0m     url: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m    184\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[Response, \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mtuple\u001B[39m[\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m]:\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 186\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rate_limiter\u001B[38;5;241m.\u001B[39mcall(\n\u001B[0;32m    187\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_requestor\u001B[38;5;241m.\u001B[39mrequest,\n\u001B[0;32m    188\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_header_callback,\n\u001B[0;32m    189\u001B[0m             method,\n\u001B[0;32m    190\u001B[0m             url,\n\u001B[0;32m    191\u001B[0m             allow_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    192\u001B[0m             data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[0;32m    193\u001B[0m             files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[0;32m    194\u001B[0m             json\u001B[38;5;241m=\u001B[39mjson,\n\u001B[0;32m    195\u001B[0m             params\u001B[38;5;241m=\u001B[39mparams,\n\u001B[0;32m    196\u001B[0m             timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    197\u001B[0m         )\n\u001B[0;32m    198\u001B[0m         log\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[0;32m    199\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m bytes) (rst-\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:rem-\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:used-\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m ratelimit) at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    200\u001B[0m             response\u001B[38;5;241m.\u001B[39mstatus_code,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    205\u001B[0m             time\u001B[38;5;241m.\u001B[39mtime(),\n\u001B[0;32m    206\u001B[0m         )\n\u001B[0;32m    207\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\prawcore\\rate_limit.py:47\u001B[0m, in \u001B[0;36mRateLimiter.call\u001B[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelay()\n\u001B[0;32m     46\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m set_header_callback()\n\u001B[1;32m---> 47\u001B[0m response \u001B[38;5;241m=\u001B[39m request_function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate(response\u001B[38;5;241m.\u001B[39mheaders)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\prawcore\\requestor.py:68\u001B[0m, in \u001B[0;36mRequestor.request\u001B[1;34m(self, timeout, *args, **kwargs)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_http\u001B[38;5;241m.\u001B[39mrequest(\u001B[38;5;241m*\u001B[39margs, timeout\u001B[38;5;241m=\u001B[39mtimeout \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:  \u001B[38;5;66;03m# noqa: BLE001\u001B[39;00m\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RequestException(exc, args, kwargs) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[0;32m    587\u001B[0m }\n\u001B[0;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[1;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m adapter\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[0;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(\n\u001B[0;32m    668\u001B[0m         method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    669\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m    670\u001B[0m         body\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mbody,\n\u001B[0;32m    671\u001B[0m         headers\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m    672\u001B[0m         redirect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    673\u001B[0m         assert_same_host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    674\u001B[0m         preload_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    675\u001B[0m         decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    676\u001B[0m         retries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries,\n\u001B[0;32m    677\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    678\u001B[0m         chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    679\u001B[0m     )\n\u001B[0;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[0;32m    786\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[0;32m    790\u001B[0m     conn,\n\u001B[0;32m    791\u001B[0m     method,\n\u001B[0;32m    792\u001B[0m     url,\n\u001B[0;32m    793\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout_obj,\n\u001B[0;32m    794\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[0;32m    795\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[0;32m    796\u001B[0m     chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    797\u001B[0m     retries\u001B[38;5;241m=\u001B[39mretries,\n\u001B[0;32m    798\u001B[0m     response_conn\u001B[38;5;241m=\u001B[39mresponse_conn,\n\u001B[0;32m    799\u001B[0m     preload_content\u001B[38;5;241m=\u001B[39mpreload_content,\n\u001B[0;32m    800\u001B[0m     decode_content\u001B[38;5;241m=\u001B[39mdecode_content,\n\u001B[0;32m    801\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse_kw,\n\u001B[0;32m    802\u001B[0m )\n\u001B[0;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[0;32m    805\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[0;32m    534\u001B[0m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 536\u001B[0m     response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:507\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    504\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresponse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPResponse\n\u001B[0;32m    506\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[1;32m--> 507\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    510\u001B[0m     assert_header_parsing(httplib_response\u001B[38;5;241m.\u001B[39mmsg)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:1428\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1427\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1428\u001B[0m         response\u001B[38;5;241m.\u001B[39mbegin()\n\u001B[0;32m   1429\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[0;32m   1430\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:331\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 331\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_status()\n\u001B[0;32m    332\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[0;32m    333\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:292\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 292\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp\u001B[38;5;241m.\u001B[39mreadline(_MAXLINE \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[0;32m    294\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\socket.py:720\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    718\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    719\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 720\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[0;32m    721\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    722\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\ssl.py:1251\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1247\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1248\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1249\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1250\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread(nbytes, buffer)\n\u001B[0;32m   1252\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\ssl.py:1103\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1101\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1102\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1103\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[0;32m   1104\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1105\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGSY5yHBPQ2r",
    "outputId": "a2bc02d4-859f-4ebd-8bce-efe1a64a119c"
   },
   "source": [
    "# @title Accessing Comments of a Specific Post\n",
    "\n",
    "# Let's get a specific post using its ID or URL (find one from the DataFrame above)\n",
    "# Example using an ID (replace with a valid ID from your fetched posts_df)\n",
    "example_post_id = None\n",
    "if 'posts_df' in locals() and not posts_df.empty:\n",
    "    example_post_id = posts_df['id'].iloc[0] # Get the ID of the first post fetched\n",
    "\n",
    "# Or use a direct URL:\n",
    "# example_post_url = \"https://www.reddit.com/r/Romania/comments/1b...../\" # Replace with a real URL\n",
    "\n",
    "if example_post_id:\n",
    "    print(f\"\\nFetching comments for post ID: {example_post_id}\")\n",
    "    try:\n",
    "        submission = reddit.submission(id=example_post_id)\n",
    "        # submission = reddit.submission(url=example_post_url) # Alternative\n",
    "\n",
    "        print(f\"Post Title: {submission.title}\")\n",
    "        print(f\"Fetching top-level comments (limit 10 for preview)...\")\n",
    "\n",
    "        # Accessing comments - initial fetch might not get all replies\n",
    "        comment_limit = 10\n",
    "        count = 0\n",
    "        submission.comments.replace_more(limit=0) # Load top-level comments, don't expand replies yet\n",
    "\n",
    "        for top_level_comment in submission.comments:\n",
    "             if count >= comment_limit:\n",
    "                 break\n",
    "             # Check if it's a Comment object (not MoreComments placeholder)\n",
    "             if isinstance(top_level_comment, praw.models.Comment):\n",
    "                 print(f\"\\n[Score: {top_level_comment.score}] Comment by u/{top_level_comment.author}\")\n",
    "                 # Limit comment body length for preview\n",
    "                 comment_body = top_level_comment.body.replace('\\n', ' ')[:150]\n",
    "                 print(f\"  '{comment_body}...'\" )\n",
    "                 count += 1\n",
    "             # elif isinstance(top_level_comment, praw.models.MoreComments):\n",
    "             #     print(\"\\n[MoreComments object - potential replies exist]\") # Placeholder for more comments\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError fetching comments for post {example_post_id}: {e}\")\n",
    "else:\n",
    "    print(\"\\nCannot fetch comments: No example_post_id found. Run the previous cell first.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "0is7TFNcPY96",
    "outputId": "2341f1c7-5610-48e2-f4e5-f2ab2020c40a"
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaq9mlHMPeGO"
   },
   "source": [
    "### Exercise 3: Subreddit Engagement Analysis 📊💬\n",
    "\n",
    "**Goal:** Analyze the relationship between post score and comment activity in a subreddit of your choice using PRAW.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1.  **Choose Subreddit & Fetch Data:**\n",
    "    *   Select a subreddit that interests you (e.g., `r/europe`, `r/AskReddit`, `r/technology`, `r/gaming`, `r/datascience`) - pick something with enough activity!).\n",
    "    *   Fetch the **top 100 posts of all time** (`.top('all', limit=100)`) from your chosen subreddit.\n",
    "    *   Create a Pandas DataFrame containing: `title`, `score`, `id`, `subreddit`, `url`, `num_comments`, `body` (selftext), and `created_time` for these 100 posts.\n",
    "2.  **Analyze Engagement:**\n",
    "    *   Sort the DataFrame by `score` in descending order.\n",
    "    *   Calculate the **average score** and **average number of comments** for the **top 10** posts (highest scores).\n",
    "    *   Calculate the **average score** and **average number of comments** for the **bottom 10** posts (lowest scores *within your top 100 sample*).\n",
    "    *   Print these averages clearly.\n",
    "    *   **Discuss:** Based on these averages, is there a clear correlation between score and comment count in this sample? Do higher-scored posts *always* get more comments? What might explain the relationship (or lack thereof)?\n",
    "3.  **Visualize Title Keywords:**\n",
    "    *   Combine all 100 post `title` strings into one large text blob.\n",
    "    *   Tokenize the text (split into words).\n",
    "    *   Convert words to lowercase.\n",
    "    *   Remove common English **stop words** (like 'the', 'a', 'is', 'in') and potentially some common Romanian ones if applicable ('de', 'si', 'o', 'un', 'cu', etc.). Also remove short words (e.g., length < 3) and non-alphabetic tokens.\n",
    "    *   Calculate the frequency of the remaining words.\n",
    "    *   Plot a bar chart showing the **top 10 most frequent words** found in the titles.\n",
    "\n",
    "**Hints:**\n",
    "*   Use `pandas` for DataFrame manipulation and calculations (`.sort_values()`, `.head()`, `.tail()`, `.mean()`).\n",
    "*   For text processing and visualization:\n",
    "    *   Use `nltk.corpus.stopwords` for English stop words (`nltk.download('stopwords')` might be needed). You might need to create a custom list for Romanian stop words.\n",
    "    *   Use `nltk.tokenize.word_tokenize` or simply `text.split()` for tokenization (`nltk.download('punkt')` might be needed for the tokenizer).\n",
    "    *   Use `collections.Counter` or `pandas.Series.value_counts()` for word frequency.\n",
    "    *   Use `matplotlib.pyplot` or `seaborn` for plotting the bar chart. Remember to label your axes!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5EhnrE4oPq6v",
    "ExecuteTime": {
     "end_time": "2025-05-08T07:00:06.694093Z",
     "start_time": "2025-05-08T07:00:01.858529Z"
    }
   },
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "CLIENT_ID = '3xKMNU3l7Uu_YNsUFUjB-g'\n",
    "CLIENT_SECRET = 'wJQ1FhdcwNPHFbMQNabUZOERLZreVg'\n",
    "USER_AGENT = 'MyRedditApp/1.0'\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")\n",
    "\n",
    "subreddit_name = \"datascience\"\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "top_posts = subreddit.top('all', limit=100)\n",
    "\n",
    "data = []\n",
    "for post in top_posts:\n",
    "    data.append({\n",
    "        \"title\": post.title,\n",
    "        \"score\": post.score,\n",
    "        \"id\": post.id,\n",
    "        \"subreddit\": post.subreddit.display_name,\n",
    "        \"url\": post.url,\n",
    "        \"num_comments\": post.num_comments,\n",
    "        \"body\": post.selftext,\n",
    "        \"created_time\": post.created_utc\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marog\\AppData\\Local\\Temp\\ipykernel_26776\\2768422527.py:17: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  top_posts = subreddit.top('all', limit=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  score      id  \\\n",
      "0                                         data siens   4115  k8nyf8   \n",
      "1                            The pain and excitement   3915  oeg6nl   \n",
      "2  Shout Out to All the Mediocre Data Scientists ...   3633  hohvgq   \n",
      "3                              Let's keep this on...   3600  xdv6nz   \n",
      "4    Guys, we’ve been doing it wrong this whole time   3462  tj3kek   \n",
      "\n",
      "     subreddit                                                url  \\\n",
      "0  datascience  https://dslntlv9vhjr4.cloudfront.net/posts_ima...   \n",
      "1  datascience                https://i.redd.it/yqnunwryjg971.jpg   \n",
      "2  datascience  https://www.reddit.com/r/datascience/comments/...   \n",
      "3  datascience                https://i.redd.it/k102dyo0yrn91.jpg   \n",
      "4  datascience                    https://i.imgur.com/TAex5zG.jpg   \n",
      "\n",
      "   num_comments                                               body  \\\n",
      "0            72                                                      \n",
      "1           175                                                      \n",
      "2           267  I've been lurking on this sub for a while now ...   \n",
      "3           121                                                      \n",
      "4           386                                                      \n",
      "\n",
      "   created_time  \n",
      "0  1.607371e+09  \n",
      "1  1.625519e+09  \n",
      "2  1.594353e+09  \n",
      "3  1.663139e+09  \n",
      "4  1.647837e+09  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T06:52:33.874636100Z",
     "start_time": "2025-05-07T07:03:02.711564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sort the DataFrame by score in descending order\n",
    "sorted_df = df.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# Calculate averages for the top 10 posts\n",
    "top_10 = sorted_df.head(10)\n",
    "top_10_avg_score = top_10[\"score\"].mean()\n",
    "top_10_avg_comments = top_10[\"num_comments\"].mean()\n",
    "\n",
    "# Calculate averages for the bottom 10 posts\n",
    "bottom_10 = sorted_df.tail(10)\n",
    "bottom_10_avg_score = bottom_10[\"score\"].mean()\n",
    "bottom_10_avg_comments = bottom_10[\"num_comments\"].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Top 10 Posts:\")\n",
    "print(f\"Average Score: {top_10_avg_score}\")\n",
    "print(f\"Average Number of Comments: {top_10_avg_comments}\\n\")\n",
    "\n",
    "print(\"Bottom 10 Posts:\")\n",
    "print(f\"Average Score: {bottom_10_avg_score}\")\n",
    "print(f\"Average Number of Comments: {bottom_10_avg_comments}\\n\")\n",
    "\n",
    "# Discussion\n",
    "print(\"Discussion: Based on the analysis, does a higher score correlate with more comments?\")\n",
    "print(\"Provide your interpretation based on the averages above.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Posts:\n",
      "Average Score: 3524.4\n",
      "Average Number of Comments: 150.5\n",
      "\n",
      "Bottom 10 Posts:\n",
      "Average Score: 1269.3\n",
      "Average Number of Comments: 70.5\n",
      "\n",
      "Discussion: Based on the analysis, does a higher score correlate with more comments?\n",
      "Provide your interpretation based on the averages above.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T06:52:33.877174400Z",
     "start_time": "2025-05-07T07:03:06.168292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# Combine all titles into a single text blob\n",
    "all_titles = \" \".join(df[\"title\"])\n",
    "\n",
    "# Tokenize words\n",
    "words = all_titles.lower().split()\n",
    "\n",
    "# Download NLTK stopwords if not already done\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Remove stop words, short words, and non-alphabetic words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# Add Romanian stop words if necessary\n",
    "stop_words.update([\"de\", \"si\", \"o\", \"un\", \"cu\"])\n",
    "cleaned_words = [\n",
    "    word for word in words if word.isalpha() and word not in stop_words and len(word) > 2\n",
    "]\n",
    "\n",
    "# Count word frequency\n",
    "word_freq = Counter(cleaned_words)\n",
    "\n",
    "# Get the 10 most common words\n",
    "most_common_words = word_freq.most_common(10)\n",
    "\n",
    "# Print the words and their frequencies\n",
    "print(\"Top 10 Words in Titles:\")\n",
    "for word, freq in most_common_words:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "# Visualize the top 10 words\n",
    "words, counts = zip(*most_common_words)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(words, counts, color=\"skyblue\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 10 Most Frequent Words in Titles\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words in Titles:\n",
      "data: 32\n",
      "science: 12\n",
      "job: 8\n",
      "scientist: 6\n",
      "like: 5\n",
      "get: 5\n",
      "time: 4\n",
      "meme: 4\n",
      "learning: 4\n",
      "think: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAH5CAYAAAC/G84fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABitUlEQVR4nO3deZyN9f//8edhzBgaYx0zk2nInr0skTC2LPmIiJAtyZKsWft80GKpD0lF+tjLXomyV0ihrCkKyb5kN5YxzMzr94ffnK9zDWIaziyP++12bnWu6zpnXudylut5vZfLZWYmAAAAAIBbOm8XAAAAAADJDUEJAAAAABwISgAAAADgQFACAAAAAAeCEgAAAAA4EJQAAAAAwIGgBAAAAAAOBCUAAAAAcCAoAQAAAIADQQlAsuFyuW7rtmrVqrtey/Tp09W8eXMVLlxY6dKlU968eW+67YULF9SjRw+FhoYqY8aMKl26tGbPnn1bf2fIkCFyuVxKly6d/vzzzwTrL168qCxZssjlcqlt27aJfDW3tmPHDg0ZMkT79u27re2nTp1603+bPn363JUak6uZM2dqzJgxt7Xtk08+qYCAAMXExHgs37Jli1wul0JCQhI8Zs2aNXK5XBo7dmxSlHtTbdu2veV7/E64XC4NGTLkHz/Pvn37bvs7Yd++fapWrZqqVavmfvylS5c0ZMiQG35fxL+Hb/c9DyBt8vF2AQAQb926dR73X3/9da1cuVLffvutx/KHHnrortfy8ccf69ixYypfvrzi4uJ09erVm27buHFjbdiwQSNGjFChQoU0c+ZMPfvss4qLi1OLFi1u6+/dd999mjJlil5//XWP5fPmzdPVq1eVIUOGf/R6bmXHjh0aOnSoqlWrdkcHy1OmTFGRIkU8loWGhiZxdcnbzJkz9euvv6pHjx5/u21ERIQWLVqkjRs36tFHH3UvX7VqlTJnzqxjx47p999/99in8Qf5ERERSV36XbNu3TrlyZPnHz9PSEhIgu+ELl266Ny5c5oxY0aCbceNG+ex7NKlSxo6dKgkeQQoALhdBCUAycb1B4+SlCtXLqVLly7B8nth2bJlSpfuWqP7k08+qV9//fWG2y1evFgrVqxwhyPp2kHt/v379corr6hZs2ZKnz793/69Zs2aadq0aRo6dKj770rSpEmT1KhRIy1cuDAJXlXSKl68uMqWLXtb2169elUul0s+Pmn3Zyc+7KxatSpBUGrYsKFWrlyplStXJghKOXPmVPHixf/R376X+z+pPq9+fn4JnitLliy6cuXKDf/GvTiBAiBtoesdgBTl9OnT6tKli+6//375+vrqwQcf1KBBgxQdHe2xncvl0ksvvaQJEyaoUKFC8vPz00MPPXTbXeKuDyu3Mn/+fN13331q2rSpx/J27drpyJEj+vHHH2/redq3b6+DBw9qxYoV7mW7du3S999/r/bt29/wMQcOHFCrVq0UFBQkPz8/FS1aVKNGjVJcXJzHduPHj1epUqV03333KSAgQEWKFNHAgQMlXeuCFF97RESEuyvT1KlTb6vuG1m1apVcLpc+/vhj9e7dW/fff7/8/Pz0xx9/SJK+/vpr1ahRQ1myZFGmTJn02GOP6ZtvvknwPIsWLVLp0qXl5+enfPny6b///a+7q2K8+O5ZN6r3Rl3Adu/erRYtWnjssw8++OCG9c+aNUuDBg1SaGiosmTJopo1a2rnzp3u7apVq6ZFixZp//79Ht3AbqZ06dLKli2bR1ewuLg4rVmzRtWqVVPVqlW1cuVK97orV65o3bp1qlatmvt5f/31VzVs2FDZsmVzd/OcNm3aHe3/qVOnqnDhwu7XP3369BvWe6v3za0493t8N7eVK1eqc+fOypkzp3LkyKHGjRvryJEjf/t8t+v6rnf79u1Trly5JElDhw51/9v8XffV23lvnjhxQh07dlRYWJj8/PyUK1cuPfbYY/r666+T7LUASB7S7qk9ACnO5cuXFRERoT179mjo0KEqWbKk1qxZo+HDh2vr1q1atGiRx/YLFy7UypUr9dprrylz5swaN26cnn32Wfn4+KhJkyZJUtOvv/6qokWLJjhTX7JkSff6SpUq/e3zFCxYUI8//rgmT56sJ554QpI0efJk5c2bVzVq1Eiw/YkTJ1SpUiVduXJFr7/+uvLmzauvvvpKffr00Z49e9zdkGbPnq0uXbqoW7du+u9//6t06dLpjz/+0I4dOyRJ9evX17BhwzRw4EB98MEHevjhhyVJ+fPn/9uaY2NjE4y3uX4/DBgwQBUrVtSHH36odOnSKSgoSJ988olat26thg0batq0acqQIYMmTJigJ554QsuWLXO/1m+++UYNGzZUxYoVNXv2bMXGxuqtt97SX3/99bd13cyOHTtUqVIlPfDAAxo1apSCg4O1bNkyvfzyyzp58qQGDx7ssf3AgQP12GOPaeLEiYqMjFS/fv3UoEED/fbbb0qfPr3GjRunjh07as+ePZo/f/7f/v106dKpSpUq+vrrrxUTEyMfHx9t3bpVZ86cUdWqVRUbG+tRw/r16xUVFeVuidq5c6cqVaqkoKAgjR07Vjly5NAnn3yitm3b6q+//lLfvn09/t6N9v/UqVPVrl07NWzYUKNGjdK5c+c0ZMgQRUdHe5wc+Lv3TWJ06NBB9evX18yZM3Xw4EG98soratWqVYKutUkhJCRES5cuVZ06dfT888+rQ4cOkuQOTzdyu+/N5557Tps3b9abb76pQoUK6ezZs9q8ebNOnTqV5K8DgJcZACRTbdq0scyZM7vvf/jhhybJ5s6d67HdyJEjTZItX77cvUyS+fv727Fjx9zLYmJirEiRIlagQIE7qqN+/foWHh5+w3UFCxa0J554IsHyI0eOmCQbNmzYLZ978ODBJslOnDhhU6ZMMT8/Pzt16pTFxMRYSEiIDRkyxMzMMmfObG3atHE/rn///ibJfvzxR4/n69y5s7lcLtu5c6eZmb300kuWNWvWW9Ywb948k2QrV6685XbxpkyZYpJueLt69aqtXLnSJFmVKlU8Hnfx4kXLnj27NWjQwGN5bGyslSpVysqXL+9eVqFCBQsNDbWoqCj3ssjISMuePbtd/9O1d+9ek2RTpkxJUKckGzx4sPv+E088YXny5LFz5855bPfSSy9ZxowZ7fTp02Zm7vrr1avnsd3cuXNNkq1bt8697FbvjRsZM2aMSbK1a9eamdmoUaMsJCTEzMx27NhhkuzXX381M7OhQ4eaJNuxY4eZmTVv3tz8/PzswIEDHs9Zt25dy5Qpk509e9ajfuf+j42NtdDQUHv44YctLi7OvXzfvn2WIUMGj9dxO++bm3Hu9/j3S5cuXTy2e+utt0ySHT169Lafu2rVqlasWLGbrqtatar7/okTJxLU4qxp7969ZnZn78377rvPevTocds1A0i56HoHIMX49ttvlTlz5gStQfHdaZxdZGrUqKHcuXO776dPn17NmjXTH3/8oUOHDiVZXbfqbnWrdU5NmzaVr6+vZsyYocWLF+vYsWM37Sr07bff6qGHHlL58uU9lrdt21Zm5j5LX758eZ09e1bPPvusFixYoJMnT952PX9n+vTp2rBhg8ft+halp59+2mP7tWvX6vTp02rTpo1iYmLct7i4ONWpU0cbNmzQxYsXdfHiRW3YsEGNGzdWxowZ3Y8PCAhQgwYNElXr5cuX9c0336hRo0bKlCmTx9+vV6+eLl++rPXr13s85l//+pfH/fhWwv379yeqBslznFL8f6tWrSpJKlq0qIKCgtzd71atWqXcuXOraNGikq79m9eoUUNhYWEez9m2bVtdunQpwcQHzv2/c+dOHTlyRC1atPB4X4aHhydo9bwb75u7sT+Tyu2+N6Vr+2bq1Kl64403tH79+ltO9AIgZSMoAUgxTp06peDg4AThIygoSD4+Pgm6vgQHByd4jvhlSdVNJkeOHDd8rtOnT0uSsmfPftvPlTlzZjVr1kyTJ0/WpEmTVLNmTYWHh99w21OnTt1wOun4Wefia3ruuec0efJk7d+/X08//bSCgoJUoUIFj7FQiVW0aFGVLVvW43Y9Z33x3eaaNGmiDBkyeNxGjhwpM9Pp06d15swZxcXF3fLf706dOnVKMTExeu+99xL87Xr16klSgjCQI0cOj/t+fn6SpKioqETVIEklSpRQzpw5tXLlSvf4pPigJElVqlTRqlWrFB0drXXr1nnMdne7/+bxnNvGr7+d/Xo33jd3Y38mldt9b0rSnDlz1KZNG02cOFEVK1ZU9uzZ1bp1ax07dsybLwHAXcAYJQApRo4cOfTjjz/KzDzC0vHjxxUTE6OcOXN6bH+jA5f4Zc6DtsQqUaKEZs2a5R5zEu+XX36RpDuerax9+/aaOHGitm3blmAK5OvlyJFDR48eTbA8fnD89fuiXbt2ateunS5evKjvvvtOgwcP1pNPPqldu3bdNIglBWegja/pvffeu+nMaLlz53bP0Harf7948S1Ozsk8nKEhW7ZsSp8+vZ577jl17dr1hn87X758t3g1ScPlcqlq1apaunSpfvrpJ509e9YjKFWtWlVDhgzRunXr3GPy4t3Jv3n837pe/Hv+dvar5L33jTfc7nszftsxY8ZozJgxOnDggBYuXKj+/fvr+PHjWrp06T2rGcDdR4sSgBSjRo0aunDhgr744guP5fGzdjknPfjmm288Bv/HxsZqzpw5yp8/f5Jc50WSGjVqpAsXLuizzz7zWD5t2jSFhoaqQoUKd/R8FStWVPv27dWoUSM1atToptvVqFFDO3bs0ObNmz2WT58+XS6X64bX3cmcObPq1q2rQYMG6cqVK9q+fbuke3dm/7HHHlPWrFm1Y8eOBC1R8TdfX19lzpxZ5cuX1+eff67Lly+7H3/+/Hl9+eWXHs+ZO3duZcyYUdu2bfNYvmDBAo/7mTJlUkREhLZs2aKSJUve8G8nJjz7+fnd8X6LiIjQxYsX9fbbbysoKMjdtU66FpROnTql9957z71tvBo1aujbb79NMFPc9OnTlSlTpr+dlrtw4cIKCQnRrFmzZGbu5fv379fatWtv+ribvW+Suzt5X9/ue9PpgQce0EsvvaRatWol+CwCSPloUQKQYrRu3VoffPCB2rRpo3379qlEiRL6/vvvNWzYMNWrV081a9b02D5nzpyqXr26/v3vf7tnvfv9999va4rwHTt2uGf4OnbsmC5duqRPP/1U0rXrtcRfs6Vu3bqqVauWOnfurMjISBUoUECzZs3S0qVL9cknn9zWNZScJk2a9Lfb9OzZU9OnT1f9+vX12muvKTw8XIsWLdK4cePUuXNnFSpUSJL0wgsvyN/fX4899phCQkJ07NgxDR8+XIGBgSpXrpyk/2v1+uijjxQQEKCMGTMqX758SdbqFu++++7Te++9pzZt2uj06dNq0qSJgoKCdOLECf388886ceKExo8fL+naxYbr1KmjWrVqqXfv3oqNjdXIkSOVOXNmdxco6VqrSatWrTR58mTlz59fpUqV0k8//aSZM2cm+PvvvvuuKleurMcff1ydO3dW3rx5df78ef3xxx/68ssvEzX7WokSJfT5559r/PjxeuSRR5QuXbq/vbZUfPiZP39+gvF2xYsXV44cOTR//nzdf//9KliwoHvd4MGD9dVXXykiIkL/+c9/lD17ds2YMUOLFi3SW2+9pcDAwFv+3XTp0un1119Xhw4d1KhRI73wwgs6e/ashgwZkqDr3e28b5K7gIAAhYeHa8GCBapRo4ayZ8+unDlz3vCiyrf73jx37pwiIiLUokULFSlSRAEBAdqwYYOWLl2qxo0b3/sXCeDu8upUEgBwC85Z78zMTp06ZZ06dbKQkBDz8fGx8PBwGzBggF2+fNljO0nWtWtXGzdunOXPn98yZMhgRYoUsRkzZtzW346fje5GN+csWufPn7eXX37ZgoODzdfX10qWLGmzZs26o79z4sSJW27nnPXOzGz//v3WokULy5Ejh2XIkMEKFy5sb7/9tsXGxrq3mTZtmkVERFju3LnN19fXQkND7ZlnnrFt27Z5PNeYMWMsX758lj59+pvOIhcvfsawDRs23HB9/Kxr8+bNu+H61atXW/369S179uyWIUMGu//++61+/foJtl+4cKGVLFnSfH197YEHHrARI0a499f1zp07Zx06dLDcuXNb5syZrUGDBrZv374b/lvt3bvX2rdvb/fff79lyJDBcuXKZZUqVbI33njjb+u/0Qx7p0+ftiZNmljWrFnN5XIlqO1mgoODTZK9//77CdY99dRTJslatmyZYN0vv/xiDRo0sMDAQPP19bVSpUol+Lf6u/0/ceJEK1iwoPn6+lqhQoVs8uTJ1qZNG49Z7273fXMjzv1+s/dLfJ23O9ui2Z3Nemdm9vXXX1uZMmXMz8/PJLk/Q85Z7+L93Xvz8uXL1qlTJytZsqRlyZLF/P39rXDhwjZ48GC7ePHibb8OACmDy+y69ncASCVcLpe6du2q999/39ulIAkNGTJEQ4cOFT9dAIC7jTFKAAAAAOBAUAIAAAAAB7reAQAAAIADLUoAAAAA4EBQAgAAAAAHghIAAAAAOKT6C87GxcXpyJEjCggIkMvl8nY5AAAAALzEzHT+/HmFhoYqXbpbtxml+qB05MgRhYWFebsMAAAAAMnEwYMHlSdPnltuk+qDUkBAgKRrOyNLlixergYAAACAt0RGRiosLMydEW4l1Qel+O52WbJkISgBAAAAuK0hOUzmAAAAAAAOBCUAAAAAcCAoAQAAAIADQQkAAAAAHAhKAAAAAOBAUAIAAAAAB4ISAAAAADgQlAAAAADAgaAEAAAAAA4EJQAAAABwICgBAAAAgANBCQAAAAAcCEoAAAAA4EBQAgAAAAAHghIAAAAAOPh4u4C0ZsSWk94u4Z7oXyant0sAAAAAEo0WJQAAAABwICgBAAAAgANBCQAAAAAcCEoAAAAA4EBQAgAAAAAHghIAAAAAOBCUAAAAAMCBoAQAAAAADgQlAAAAAHAgKAEAAACAA0EJAAAAABwISgAAAADgQFACAAAAAAeCEgAAAAA4EJQAAAAAwIGgBAAAAAAOBCUAAAAAcCAoAQAAAICDV4PS+PHjVbJkSWXJkkVZsmRRxYoVtWTJEvd6M9OQIUMUGhoqf39/VatWTdu3b/dixQAAAADSAq8GpTx58mjEiBHauHGjNm7cqOrVq6thw4buMPTWW29p9OjRev/997VhwwYFBwerVq1aOn/+vDfLBgAAAJDKeTUoNWjQQPXq1VOhQoVUqFAhvfnmm7rvvvu0fv16mZnGjBmjQYMGqXHjxipevLimTZumS5cuaebMmd4sGwAAAEAql2zGKMXGxmr27Nm6ePGiKlasqL179+rYsWOqXbu2exs/Pz9VrVpVa9euvenzREdHKzIy0uMGAAAAAHfC60Hpl19+0X333Sc/Pz916tRJ8+fP10MPPaRjx45JknLnzu2xfe7cud3rbmT48OEKDAx038LCwu5q/QAAAABSH68HpcKFC2vr1q1av369OnfurDZt2mjHjh3u9S6Xy2N7M0uw7HoDBgzQuXPn3LeDBw/etdoBAAAApE4+3i7A19dXBQoUkCSVLVtWGzZs0Lvvvqt+/fpJko4dO6aQkBD39sePH0/QynQ9Pz8/+fn53d2iAQAAAKRqXm9RcjIzRUdHK1++fAoODtaKFSvc665cuaLVq1erUqVKXqwQAAAAQGrn1RalgQMHqm7dugoLC9P58+c1e/ZsrVq1SkuXLpXL5VKPHj00bNgwFSxYUAULFtSwYcOUKVMmtWjRwptlAwAAAEjlvBqU/vrrLz333HM6evSoAgMDVbJkSS1dulS1atWSJPXt21dRUVHq0qWLzpw5owoVKmj58uUKCAjwZtkAAAAAUjmXmZm3i7ibIiMjFRgYqHPnzilLlizeLkcjtpz0dgn3RP8yOb1dAgAAAODhTrJBshujBAAAAADeRlACAAAAAAeCEgAAAAA4EJQAAAAAwIGgBAAAAAAOBCUAAAAAcCAoAQAAAIADQQkAAAAAHAhKAAAAAOBAUAIAAAAAB4ISAAAAADgQlAAAAADAgaAEAAAAAA4EJQAAAABwICgBAAAAgANBCQAAAAAcCEoAAAAA4EBQAgAAAAAHghIAAAAAOBCUAAAAAMCBoAQAAAAADgQlAAAAAHAgKAEAAACAA0EJAAAAABwISgAAAADgQFACAAAAAAeCEgAAAAA4EJQAAAAAwIGgBAAAAAAOBCUAAAAAcCAoAQAAAIADQQkAAAAAHAhKAAAAAOBAUAIAAAAAB4ISAAAAADgQlAAAAADAgaAEAAAAAA4EJQAAAABwICgBAAAAgANBCQAAAAAcCEoAAAAA4EBQAgAAAAAHghIAAAAAOBCUAAAAAMCBoAQAAAAADgQlAAAAAHDwalAaPny4ypUrp4CAAAUFBempp57Szp07PbZp27atXC6Xx+3RRx/1UsUAAAAA0gKvBqXVq1era9euWr9+vVasWKGYmBjVrl1bFy9e9NiuTp06Onr0qPu2ePFiL1UMAAAAIC3w8eYfX7p0qcf9KVOmKCgoSJs2bVKVKlXcy/38/BQcHHyvywMAAACQRiWrMUrnzp2TJGXPnt1j+apVqxQUFKRChQrphRde0PHjx2/6HNHR0YqMjPS4AQAAAMCdSDZByczUq1cvVa5cWcWLF3cvr1u3rmbMmKFvv/1Wo0aN0oYNG1S9enVFR0ff8HmGDx+uwMBA9y0sLOxevQQAAAAAqYTLzMzbRUhS165dtWjRIn3//ffKkyfPTbc7evSowsPDNXv2bDVu3DjB+ujoaI8QFRkZqbCwMJ07d05ZsmS5K7XfiRFbTnq7hHuif5mc3i4BAAAA8BAZGanAwMDbygZeHaMUr1u3blq4cKG+++67W4YkSQoJCVF4eLh27959w/V+fn7y8/O7G2UCAAAASCO8GpTMTN26ddP8+fO1atUq5cuX728fc+rUKR08eFAhISH3oEIAAAAAaZFXxyh17dpVn3zyiWbOnKmAgAAdO3ZMx44dU1RUlCTpwoUL6tOnj9atW6d9+/Zp1apVatCggXLmzKlGjRp5s3QAAAAAqZhXW5TGjx8vSapWrZrH8ilTpqht27ZKnz69fvnlF02fPl1nz55VSEiIIiIiNGfOHAUEBHihYgAAAABpgde73t2Kv7+/li1bdo+qAQAAAIBrks304AAAAACQXBCUAAAAAMCBoAQAAAAADgQlAAAAAHAgKAEAAACAA0EJAAAAABwISgAAAADgQFACAAAAAAeCEgAAAAA4EJQAAAAAwIGgBAAAAAAOBCUAAAAAcCAoAQAAAIADQQkAAAAAHAhKAAAAAOBAUAIAAAAAB4ISAAAAADgQlAAAAADAgaAEAAAAAA4EJQAAAABwICgBAAAAgANBCQAAAAAcCEoAAAAA4EBQAgAAAAAHghIAAAAAOBCUAAAAAMCBoAQAAAAADgQlAAAAAHAgKAEAAACAA0EJAAAAABwISgAAAADgQFACAAAAAAeCEgAAAAA4EJQAAAAAwIGgBAAAAAAOBCUAAAAAcCAoAQAAAIADQQkAAAAAHAhKAAAAAOBAUAIAAAAAB4ISAAAAADgQlAAAAADAgaAEAAAAAA4EJQAAAABwICgBAAAAgANBCQAAAAAcCEoAAAAA4ODVoDR8+HCVK1dOAQEBCgoK0lNPPaWdO3d6bGNmGjJkiEJDQ+Xv769q1app+/btXqoYAAAAQFrg1aC0evVqde3aVevXr9eKFSsUExOj2rVr6+LFi+5t3nrrLY0ePVrvv/++NmzYoODgYNWqVUvnz5/3YuUAAAAAUjOXmZm3i4h34sQJBQUFafXq1apSpYrMTKGhoerRo4f69esnSYqOjlbu3Lk1cuRIvfjii3/7nJGRkQoMDNS5c+eUJUuWu/0S/taILSe9XcI90b9MTm+XAAAAAHi4k2yQrMYonTt3TpKUPXt2SdLevXt17Ngx1a5d272Nn5+fqlatqrVr197wOaKjoxUZGelxAwAAAIA7kWyCkpmpV69eqly5sooXLy5JOnbsmCQpd+7cHtvmzp3bvc5p+PDhCgwMdN/CwsLubuEAAAAAUp1kE5Reeuklbdu2TbNmzUqwzuVyedw3swTL4g0YMEDnzp1z3w4ePHhX6gUAAACQevl4uwBJ6tatmxYuXKjvvvtOefLkcS8PDg6WdK1lKSQkxL38+PHjCVqZ4vn5+cnPz+/uFgwAAAAgVfNqi5KZ6aWXXtLnn3+ub7/9Vvny5fNYny9fPgUHB2vFihXuZVeuXNHq1atVqVKle10uAAAAgDQiUS1Ke/fuTRBqEqNr166aOXOmFixYoICAAPe4o8DAQPn7+8vlcqlHjx4aNmyYChYsqIIFC2rYsGHKlCmTWrRo8Y//PgAAAADcSKKCUoECBVSlShU9//zzatKkiTJmzJioPz5+/HhJUrVq1TyWT5kyRW3btpUk9e3bV1FRUerSpYvOnDmjChUqaPny5QoICEjU3wQAAACAv5Oo6yj9+uuvmjx5smbMmKHo6Gg1a9ZMzz//vMqXL383avxHuI6Sd3AdJQAAACQ3d/06SsWLF9fo0aN1+PBhTZkyRceOHVPlypVVrFgxjR49WidOnEhU4QAAAACQHPyjyRx8fHzUqFEjzZ07VyNHjtSePXvUp08f5cmTR61bt9bRo0eTqk4AAAAAuGf+UVDauHGjunTpopCQEI0ePVp9+vTRnj179O233+rw4cNq2LBhUtUJAAAAAPdMoiZzGD16tKZMmaKdO3eqXr16mj59uurVq6d06a7lrnz58mnChAkqUqRIkhYLAAAAAPdCooLS+PHj1b59e7Vr1859UVinBx54QJMmTfpHxQEAAACANyQqKO3evftvt/H19VWbNm0S8/QAAAAA4FWJGqM0ZcoUzZs3L8HyefPmadq0af+4KAAAAADwpkQFpREjRihnzoTXyQkKCtKwYcP+cVEAAAAA4E2JCkr79+9Xvnz5EiwPDw/XgQMH/nFRAAAAAOBNiQpKQUFB2rZtW4LlP//8s3LkyPGPiwIAAAAAb0pUUGrevLlefvllrVy5UrGxsYqNjdW3336r7t27q3nz5kldIwAAAADcU4ma9e6NN97Q/v37VaNGDfn4XHuKuLg4tW7dmjFKAAAAAFK8RAUlX19fzZkzR6+//rp+/vln+fv7q0SJEgoPD0/q+gAAAADgnktUUIpXqFAhFSpUKKlqAQAAAIBkIVFBKTY2VlOnTtU333yj48ePKy4uzmP9t99+myTFAQAAAIA3JCoode/eXVOnTlX9+vVVvHhxuVyupK4LAAAAALwmUUFp9uzZmjt3rurVq5fU9QAAAACA1yVqenBfX18VKFAgqWsBAAAAgGQhUUGpd+/eevfdd2VmSV0PAAAAAHhdorreff/991q5cqWWLFmiYsWKKUOGDB7rP//88yQpDgAAAAC8IVFBKWvWrGrUqFFS1wIAAAAAyUKigtKUKVOSug4AAAAASDYSNUZJkmJiYvT1119rwoQJOn/+vCTpyJEjunDhQpIVBwAAAADekKgWpf3796tOnTo6cOCAoqOjVatWLQUEBOitt97S5cuX9eGHHyZ1nQAAAABwzySqRal79+4qW7aszpw5I39/f/fyRo0a6Ztvvkmy4gAAAADAGxI9690PP/wgX19fj+Xh4eE6fPhwkhQGAAAAAN6SqBaluLg4xcbGJlh+6NAhBQQE/OOiAAAAAMCbEhWUatWqpTFjxrjvu1wuXbhwQYMHD1a9evWSqjYAAAAA8IpEdb175513FBERoYceekiXL19WixYttHv3buXMmVOzZs1K6hoBAAAA4J5KVFAKDQ3V1q1bNWvWLG3evFlxcXF6/vnn1bJlS4/JHQAAAAAgJUpUUJIkf39/tW/fXu3bt0/KegAAAADA6xIVlKZPn37L9a1bt05UMQAAAACQHCQqKHXv3t3j/tWrV3Xp0iX5+voqU6ZMBCUAAAAAKVqiZr07c+aMx+3ChQvauXOnKleuzGQOAAAAAFK8RAWlGylYsKBGjBiRoLUJAAAAAFKaJAtKkpQ+fXodOXIkKZ8SAAAAAO65RI1RWrhwocd9M9PRo0f1/vvv67HHHkuSwgAAAADAWxIVlJ566imP+y6XS7ly5VL16tU1atSopKgLAAAAALwmUUEpLi4uqesAAAAAgGQjSccoAQAAAEBqkKgWpV69et32tqNHj07MnwAAAAAAr0lUUNqyZYs2b96smJgYFS5cWJK0a9cupU+fXg8//LB7O5fLlTRVAgAAAMA9lKig1KBBAwUEBGjatGnKli2bpGsXoW3Xrp0ef/xx9e7dO0mLBAAAAIB7KVFjlEaNGqXhw4e7Q5IkZcuWTW+88Qaz3gEAAABI8RIVlCIjI/XXX38lWH78+HGdP3/+HxcFAAAAAN6UqKDUqFEjtWvXTp9++qkOHTqkQ4cO6dNPP9Xzzz+vxo0bJ3WNAAAAAHBPJSooffjhh6pfv75atWql8PBwhYeHq2XLlqpbt67GjRt328/z3XffqUGDBgoNDZXL5dIXX3zhsb5t27ZyuVwet0cffTQxJQMAAADAbUvUZA6ZMmXSuHHj9Pbbb2vPnj0yMxUoUECZM2e+o+e5ePGiSpUqpXbt2unpp5++4TZ16tTRlClT3Pd9fX0TUzIAAAAA3LZEBaV4R48e1dGjR1WlShX5+/vLzO5oSvC6deuqbt26t9zGz89PwcHB/6RMAAAAALgjiep6d+rUKdWoUUOFChVSvXr1dPToUUlShw4dknxq8FWrVikoKEiFChXSCy+8oOPHj99y++joaEVGRnrcAAAAAOBOJCoo9ezZUxkyZNCBAweUKVMm9/JmzZpp6dKlSVZc3bp1NWPGDH377bcaNWqUNmzYoOrVqys6Ovqmjxk+fLgCAwPdt7CwsCSrBwAAAEDakKiud8uXL9eyZcuUJ08ej+UFCxbU/v37k6Qw6Vrwile8eHGVLVtW4eHhWrRo0U1n1xswYIB69erlvh8ZGUlYAgAAAHBHEhWULl686NGSFO/kyZPy8/P7x0XdTEhIiMLDw7V79+6bbuPn53dXawAAAACQ+iWq612VKlU0ffp0932Xy6W4uDi9/fbbioiISLLinE6dOqWDBw8qJCTkrv0NAAAAAEhUi9Lbb7+tatWqaePGjbpy5Yr69u2r7du36/Tp0/rhhx9u+3kuXLigP/74w31/79692rp1q7Jnz67s2bNryJAhevrppxUSEqJ9+/Zp4MCBypkzpxo1apSYsgEAAADgtiSqRemhhx7Stm3bVL58edWqVUsXL15U48aNtWXLFuXPn/+2n2fjxo0qU6aMypQpI0nq1auXypQpo//85z9Knz69fvnlFzVs2FCFChVSmzZtVKhQIa1bt04BAQGJKRsAAAAAbssdtyhdvXpVtWvX1oQJEzR06NB/9MerVasmM7vp+mXLlv2j5wcAAACAxLjjFqUMGTLo119/vaMLywIAAABASpKornetW7fWpEmTkroWAAAAAEgWEjWZw5UrVzRx4kStWLFCZcuWVebMmT3Wjx49OkmKAwAAAABvuKOg9Oeffypv3rz69ddf9fDDD0uSdu3a5bENXfIAAAAApHR3FJQKFiyoo0ePauXKlZKkZs2aaezYscqdO/ddKQ4AAAAAvOGOxig5Z6hbsmSJLl68mKQFAQAAAIC3JWoyh3i3mtobAAAAAFKqOwpKLpcrwRgkxiQBAAAASG3uaIySmalt27by8/OTJF2+fFmdOnVKMOvd559/nnQVAgAAAMA9dkdBqU2bNh73W7VqlaTFAAAAAEBycEdBacqUKXerDgAAAABINv7RZA4AAAAAkBoRlAAAAADAgaAEAAAAAA4EJQAAAABwICgBAAAAgANBCQAAAAAcCEoAAAAA4EBQAgAAAAAHghIAAAAAOBCUAAAAAMCBoAQAAAAADgQlAAAAAHAgKAEAAACAA0EJAAAAABwISgAAAADgQFACAAAAAAcfbxcAXG/ElpPeLuGe6F8mp7dLAAAAwC3QogQAAAAADgQlAAAAAHAgKAEAAACAA0EJAAAAABwISgAAAADgQFACAAAAAAeCEgAAAAA4EJQAAAAAwIGgBAAAAAAOBCUAAAAAcCAoAQAAAIADQQkAAAAAHAhKAAAAAOBAUAIAAAAAB4ISAAAAADgQlAAAAADAgaAEAAAAAA4EJQAAAABw8GpQ+u6779SgQQOFhobK5XLpiy++8FhvZhoyZIhCQ0Pl7++vatWqafv27d4pFgAAAECa4dWgdPHiRZUqVUrvv//+Dde/9dZbGj16tN5//31t2LBBwcHBqlWrls6fP3+PKwUAAACQlvh484/XrVtXdevWveE6M9OYMWM0aNAgNW7cWJI0bdo05c6dWzNnztSLL754L0sFAAAAkIYk2zFKe/fu1bFjx1S7dm33Mj8/P1WtWlVr16696eOio6MVGRnpcQMAAACAO5Fsg9KxY8ckSblz5/ZYnjt3bve6Gxk+fLgCAwPdt7CwsLtaJwAAAIDUJ9kGpXgul8vjvpklWHa9AQMG6Ny5c+7bwYMH73aJAAAAAFIZr45RupXg4GBJ11qWQkJC3MuPHz+eoJXpen5+fvLz87vr9QEAAABIvZJti1K+fPkUHBysFStWuJdduXJFq1evVqVKlbxYGQAAAIDUzqstShcuXNAff/zhvr93715t3bpV2bNn1wMPPKAePXpo2LBhKliwoAoWLKhhw4YpU6ZMatGihRerBgAAAJDaeTUobdy4UREREe77vXr1kiS1adNGU6dOVd++fRUVFaUuXbrozJkzqlChgpYvX66AgABvlQwAAAAgDfBqUKpWrZrM7KbrXS6XhgwZoiFDhty7ogAAAACkecl2jBIAAAAAeAtBCQAAAAAcCEoAAAAA4EBQAgAAAAAHghIAAAAAOBCUAAAAAMCBoAQAAAAADgQlAAAAAHAgKAEAAACAA0EJAAAAABwISgAAAADgQFACAAAAAAeCEgAAAAA4EJQAAAAAwIGgBAAAAAAOBCUAAAAAcPDxdgEAbt+ILSe9XcI90b9MTm+XAAAA0jhalAAAAADAgaAEAAAAAA4EJQAAAABwICgBAAAAgANBCQAAAAAcCEoAAAAA4EBQAgAAAAAHghIAAAAAOBCUAAAAAMCBoAQAAAAADgQlAAAAAHAgKAEAAACAA0EJAAAAABwISgAAAADgQFACAAAAAAeCEgAAAAA4EJQAAAAAwIGgBAAAAAAOBCUAAAAAcCAoAQAAAIADQQkAAAAAHAhKAAAAAODg4+0CACCpjNhy0tsl3HX9y+T0dgkAAKQJtCgBAAAAgANBCQAAAAAcCEoAAAAA4EBQAgAAAAAHghIAAAAAOBCUAAAAAMCBoAQAAAAADsk6KA0ZMkQul8vjFhwc7O2yAAAAAKRyyf6Cs8WKFdPXX3/tvp8+fXovVgMAAAAgLUj2QcnHx4dWJAAAAAD3VLLueidJu3fvVmhoqPLly6fmzZvrzz//vOX20dHRioyM9LgBAAAAwJ1I1i1KFSpU0PTp01WoUCH99ddfeuONN1SpUiVt375dOXLkuOFjhg8frqFDh97jSgEg+Rux5aS3S7jr+pfJmejHsn8AANdL1i1KdevW1dNPP60SJUqoZs2aWrRokSRp2rRpN33MgAEDdO7cOfft4MGD96pcAAAAAKlEsm5RcsqcObNKlCih3bt333QbPz8/+fn53cOqAAAAAKQ2ybpFySk6Olq//fabQkJCvF0KAAAAgFQsWQelPn36aPXq1dq7d69+/PFHNWnSRJGRkWrTpo23SwMAAACQiiXrrneHDh3Ss88+q5MnTypXrlx69NFHtX79eoWHh3u7NAAAAACpWLIOSrNnz/Z2CQAAAADSoGTd9Q4AAAAAvIGgBAAAAAAOBCUAAAAAcCAoAQAAAIBDsp7MAQAAJA8jtpz0dgl3Xf8yORP9WPbPzaWFfSP9s/cPkidalAAAAADAgaAEAAAAAA4EJQAAAABwICgBAAAAgANBCQAAAAAcCEoAAAAA4EBQAgAAAAAHghIAAAAAOBCUAAAAAMCBoAQAAAAADgQlAAAAAHDw8XYBAAAAQFo1YstJb5dwT/Qvk9PbJdwxWpQAAAAAwIGgBAAAAAAOBCUAAAAAcCAoAQAAAIADQQkAAAAAHAhKAAAAAOBAUAIAAAAAB4ISAAAAADgQlAAAAADAgaAEAAAAAA4EJQAAAABwICgBAAAAgANBCQAAAAAcCEoAAAAA4EBQAgAAAAAHghIAAAAAOBCUAAAAAMCBoAQAAAAADgQlAAAAAHAgKAEAAACAA0EJAAAAABwISgAAAADgQFACAAAAAAeCEgAAAAA4EJQAAAAAwIGgBAAAAAAOBCUAAAAAcCAoAQAAAIADQQkAAAAAHFJEUBo3bpzy5cunjBkz6pFHHtGaNWu8XRIAAACAVCzZB6U5c+aoR48eGjRokLZs2aLHH39cdevW1YEDB7xdGgAAAIBUKtkHpdGjR+v5559Xhw4dVLRoUY0ZM0ZhYWEaP368t0sDAAAAkEr5eLuAW7ly5Yo2bdqk/v37eyyvXbu21q5de8PHREdHKzo62n3/3LlzkqTIyMi7V+gduHzhvLdLuCciI30T9Tj2z62xf24tLeyfxO4bif3zd9g/t8b+uTX2z82lhX0jsX/+zj/5fCWl+ExgZn+/sSVjhw8fNkn2ww8/eCx/8803rVChQjd8zODBg00SN27cuHHjxo0bN27cuN3wdvDgwb/NIsm6RSmey+XyuG9mCZbFGzBggHr16uW+HxcXp9OnTytHjhw3fUxqFhkZqbCwMB08eFBZsmTxdjnJDvvn1tg/t8b+uTX2z82xb26N/XNr7J9bY//cWlrfP2am8+fPKzQ09G+3TdZBKWfOnEqfPr2OHTvmsfz48ePKnTv3DR/j5+cnPz8/j2VZs2a9WyWmGFmyZEmTH4bbxf65NfbPrbF/bo39c3Psm1tj/9wa++fW2D+3lpb3T2Bg4G1tl6wnc/D19dUjjzyiFStWeCxfsWKFKlWq5KWqAAAAAKR2ybpFSZJ69eql5557TmXLllXFihX10Ucf6cCBA+rUqZO3SwMAAACQSiX7oNSsWTOdOnVKr732mo4eParixYtr8eLFCg8P93ZpKYKfn58GDx6coDsirmH/3Br759bYP7fG/rk59s2tsX9ujf1za+yfW2P/3D6X2e3MjQcAAAAAaUeyHqMEAAAAAN5AUAIAAAAAB4ISAAAAADgQlAAAAADAgaAEAAAAAA4EJQDwAiYchXTtfRAbG+vtMgAAN0BQAnBDHMAlvbi4OPd/XS6XJGnXrl06c+aMN8uCF+3Zs0fp06eXJE2ePFnr1q3zckVA2uU8gZUWT2jFv+arV696uZLkgaCUxlz/AYiJifFyNSlPWvnSjI2Nlcvlch/ATZgwQf369VOXLl20ZcsWnTt3zssVpkzp0qXTvn37VKdOHUnSl19+qXr16unIkSNeruzuSyufnTuxbds2FS1aVJ988on69++v3r17Kzg42NtlJUvx75/z58/r5MmTN1yH28P+urn4E1h79uyRmcnlcqWp/RX/mhcuXKhRo0a5T+6lZQSlNCT+A7B8+XK1aNFCtWvXVrdu3XTq1Clvl5YixO+/H374QcOGDVOvXr301VdfebusJNe1a1cVK1bMHaRfeeUV9evXT7///rvWrFmjJ598UmPHjtXRo0e9XGnKtH//fh08eFDFihXTU089pTfffFPFihXzdll3TfxBxunTp3Xx4kX3QW5aOvi4maCgIA0ePFgdO3bUhx9+qB07dihfvny05Dpcf/DWoEEDlS1bVg0aNNCoUaMUFRXlPrjFjf3xxx/asWOH1q5dK0nsrxu4PhBMnz5dzZs31+LFi9NEWFq2bJm2b98u6f/eG59++ql8fX2VLh0xgT2QhrhcLi1YsEBNmzZVrly59Oyzz2rOnDl68cUXtXXrVm+Xl+y5XC59/vnnatiwoX766SdduHBB//rXvzRgwABFRkZ6u7wk06pVK8XFxal69eo6ceKE/vzzT61YsUILFizQL7/8oueff16fffaZ5syZo5iYmFT9A3I3VK1aVa1bt9Zvv/2m/Pnzq1mzZpKUKlt44w8yvvrqKzVt2lSPP/64atWqpTlz5nCwJik4OFi5c+fW5cuXdfXqVa1YsUKSlD59es7kXsflcmnJkiV69tlnVbduXX311VcKDAzU8OHD9d1333m7vGRt/vz5evLJJ9WsWTM9/fTTaty4sX7//Xdvl5WsxMXFuQPBokWLtG/fPm3ZskXDhg3TihUrUm1YMjP9/vvvatSokcaOHatdu3a51x05ciRV/iYliiHN2L59uxUpUsTef/99MzOLjIy0kJAQ8/X1tYoVK9rWrVu9XGHytmvXLsubN699+OGHZmZ24cIF8/X1tX79+nm5sqS3ceNGe/DBB61QoUJWrlw5+/PPPz3W9+7d2/LkyWPnz5/3UoUpT1xcnJmZxcbG2qeffmpDhgyxcuXKWfny5e3SpUtmZnblyhVvlnhXfPXVV5YxY0Z75513bM2aNfbyyy+by+Wyn376yduleUVsbKyZ/d/74fDhw7Zx40YbOnSo3Xfffe7vl/j1aV1sbKxFRUVZ06ZNbciQIWZmdvbsWcuTJ49169bNYzt4Wrlypd133302ceJEu3Tpkn3zzTfmcrls5syZ3i4tWerfv78FBQXZ6NGjbejQofbAAw9YuXLlbOnSpe7PY2r8XM6ePdvCw8Otc+fOtmPHDjMzq1Onjk2cONHMPL+zUuPr/zsEpVTo+g/09T8emzdvtqFDh1pMTIwdOnTI8uXLZy+//LIdOHDAcubMaU899ZRt2LDBW2Une5s3b7YqVaqYmdkff/xh999/v3Xs2NG9fs+ePd4q7a7YsGGDVapUyXx9fe2XX34xM7Po6Ggzu3agEhgYaF988YU3S0wx4j+Tq1atso8//tiOHTtmZmZff/21lSlTxsqXL+/et2Zma9eutTNnznij1CR19epVe/bZZ23o0KFmZrZ//37Lnz+/x+fGLHUefNzI9d/Hv/32m61fv95OnjxpcXFxduHCBevfv78FBATY//73P/d2w4YN43vZzKpXr24rV660Q4cOWWhoqMd7aNGiRWk2eN/Km2++6d5Pu3fvvuFnD9f89ttvdv/999uXX37pXnbs2DErUaKElS5d2pYtW5bqwtL1r2Pu3LmWJ08ee+GFF2zHjh3WqFEjW7FiRYLHXLhw4V6WmCwQlFKpkydPuv9/xYoVtmDBAouJibHt27dbXFyctWzZ0lq1auU+k129enVzuVxWu3Ztu3z5srfKTtaWLFliefPmtbVr11q+fPmsY8eOFhMTY2Zma9assfr169uBAwe8XGXi3Ohs7NWrV23Dhg1WqFAhK1++vJ07d869bs+ePRYWFmZff/31vSwzRYr/Mfrss88sMDDQBg4caH/88YeZXdvH33zzjZUqVcrKli1re/bssUGDBlmRIkXcYSqluf7HNyoqygoXLmyLFy+2s2fPuk8uxG8zYcIE9xnM1O76/TJw4EArWrSoBQcHW9myZa1Tp072119/2YkTJ+zVV181Pz8/69atm9WsWdMKFSrk/p5Ji2JiYiw6OtoeffRRa9eunftg/+rVq2ZmdurUKWvWrJlNnDgx1RzAJpWGDRtanz597PLly5YnTx6Pz977779vn3zyiZcrTD7++OMPj9+0+BNXR48etWzZslmNGjVs6dKl3izxrrj+u2XOnDkWFhZmnTt3tuDgYMuVK5c1aNDAateubVWqVLGaNWtahw4dLCoqyosV33sEpVTozJkzFhQUZMOHD7eFCxdaunTpbOHChe71V69etapVq9o777zjXtazZ09bt26d+wAurYv/MdmyZYstW7bMrl69aqdOnbK6deta5syZrXnz5h7bDRgwwCIiIuz48eNeqzmxrg9J69evt6VLl9q2bdvc3eo2bdpkDz74oJUpU8amTp1qCxcutHr16lnp0qXT9AHcnfjuu+8sMDDQpk6desP169evtzJlylju3Lktb968Kf7s+LJly+yHH34wM7MOHTpYly5dLE+ePNapUyf3AW5kZKS1aNHCxo4dm6a6Tf33v/+1oKAg++abb8zMrFWrVpYzZ073/jp58qR98MEHVqlSJWvRooW7O2Za2Ufx36mnTp2y2NhY9+tfsmSJZcuWzUqXLu2x/aBBgyx//vyprkU/KUydOtUqVapkOXPmtM6dO5vZ/+3fjh07WqdOndLkidEbBeqTJ09aaGioDRgwwL3s6tWrFhMTY4899piFhIRYRESE7du376bPkZLE1+/8958xY4YFBwdb8eLFrW3btjZ9+nQbO3asDR482EaNGmXbtm3zRrleRVBKhS5fvmwff/yx+fr6mp+fn82ZM8fM/u+DERUVZcWKFbOnnnrKli9fbn369LGgoKAUeZB/N8Tvp08//dSCg4Pttddes127dpmZ2XvvvWeFChWy5557zn7++Wf78ccf7ZVXXrGsWbOmuC8QZ3/jvn37WmhoqOXNm9d8fX2tWbNm7oO5TZs2WcmSJc3lctlLL71k/fv3dx/AEJY89e3b1+bOneuxbNSoUfavf/3LzMwuXrxoy5Yts2eeecZatmxpkydPNjOzS5cu2erVq+3w4cP3vOZ/YufOne6zr7GxsXb27Fl76KGHbNmyZWZ2LRgEBARYlSpV3N0J4+LibMCAAZY/f/4E499Sq9jYWLt06ZI1aNDAxo0bZ2ZmixcvtoCAAJswYYKZXTuLHX/gcvnyZffnMz5cphVffPGFVaxY0cqWLWsjR450n8AbMWKEpUuXzpo2bWqdO3e25557zrJmzWqbN2/2csXJw8GDB92/VWbXxppWrFjRihQpYqtWrTIzs3PnztmgQYMsNDTUfv/9d2+V6jXXn3DYt2+fnT9/3t2dbMqUKebj42Njx451bxMTE2Pt27e3VatWWc6cOVPFmOT475WlS5fa008/bW3atLEpU6a4v2fmzZtnefLksa5du6bYXjJJiaCUSm3cuNFcLpe5XC5766233MvjPwg///yz5cqVyx588EHLly8fPzQO3333nWXJksU+/PBDd/fEeO+8845Vr17d0qdPb6VKlbJHHnnEtmzZ4p1CE+ngwYMe9ydMmGC5cuWy1atX25kzZ+yrr76y2rVrW/369W3t2rVmdm3MUmhoqPvMpFnaO4D7O2fOnLH+/fvbzz//7LF80KBB9uCDD9rs2bPtySeftLp169oTTzxhzZs3t4cffjjFtuTOnz/fXC6Xffrpp+7gHBUVZQ8++KCtXr3avV23bt2sUKFC1rBhQ+vZs6c1a9bMsmXLluI+N3fqRmedK1asaNu2bbNly5Z5TN4QHR1tEydOtNWrV3sczKX0M9d3auvWrZYjRw574403rHXr1laxYkVr2rSp7d6928zMli9fbvXq1bOGDRta9+7d7bfffvNyxcnDZ599ZiEhIZY3b14rWbKku1V68eLFVqVKFcuXL59VqFDBqlWrZqGhoWn+N//f//63PfTQQ1akSBHr06ePu0Xy9ddfN5fLZc2aNbPevXtblSpVrFixYmZm1rJlS2vUqJE3y04yq1evNh8fH3vxxRetfPnyVqFCBevSpYv7e3zGjBmWN29ea9mype3cudPL1XoXQSmVif+BvXr1qn3//fc2bdo0S58+vb322mvubeIPbi9dumR79+6lJek68Qclffv2taZNm3qsu36wfVRUlK1fv94OHjxop06duqc1/lNdunSxV155xcz+rzWoffv21rp1a4/tVq5caQ8//LD16dPHzK69b3777TdakP5G/Odr2bJl9vHHH5vZtVakypUrW9GiRa1Nmzbulrq1a9da8eLFbf/+/V6r95965plnLEeOHPb5559bVFSUXbp0yYoWLWo7d+70OMj/4IMPrF27dhYREWE9evRI9WOTrn/ts2bNsvfee8/MzOrVq2dFihSxwMBAmzRpknubw4cPW/Xq1d0tjGnJ9fvqu+++sx49erjvf/LJJ1atWjVr3Lixbd++3cz+7zOWVroj/p29e/dakSJFbPTo0bZ8+XKrVauW5cyZ05YsWWJm12a8nT17tr388ss2efLkNNdN0dl7Yu7cuRYUFGRz58617t27W7Vq1axu3bruE1aLFy+2mjVrWp06daxly5bu8FCnTh2P92ZKcv1n5Y8//rC33nrLxowZY2bXfp9Gjhxp5cqVs44dO7pf79SpU61YsWJ29OhRr9ScXBCUUonru9Vd/4G4ePGijRs3ztKnT29vvvmme/n//vc/Ziy7hVatWlnjxo3NLOGP8datW1N0S8oXX3zh/iKMn/Sjffv27jNl17/eUaNGWfbs2e306dMez0FY+j83m2Wya9eu5nK5bPr06WZ27eDOGYgGDhxo5cqVsxMnTty7gpPAjBkzPC4n8Oyzz1pgYKDNmzfP/vzzTytRooQdOnTIixV61/Xvg19//dXKlCljZcqUsfnz59vmzZvt4YcftpIlS5rZtS52Z86csbp161rlypXT3Gcr/vOzZs0ae++992zAgAHWs2dPj23iw1LTpk09WiHTWmvbzZw5c8YGDBjg8b5r0KCB5ciRw5YsWUKgvM6SJUvslVdesSlTpriXzZ0712rUqGFPPPGEu4Xy+rE7UVFR1rdvX8udO3eK6644evRoj5NSv/32m1WpUsXy5s1rs2fPdi8/f/68Oyx17tzZfWL4+kmc0iqCUioQ/2OxbNkye+qpp6x27drWsmVL98H8lStXbNy4cebj42OtW7e2Tp06WcaMGemycAPx+/KVV16xkJAQu3jxosfyc+fOWb9+/Ty6FaUUzoOKadOm2RNPPGGHDx+2uXPnmsvlsjVr1nhsM2fOHHv00UctMjLyXpaaYlw/8Dy+ZXb+/Pn2559/2vHjx+2VV16xgIAAd1iKt3DhQuvdu7cFBgamuO5ne/bssSJFirgHNcd79tlnLVeuXPbRRx9Z3rx5rWnTpjZ48GAbOXKkDRw40Hr37m2ffPJJmroWR58+fezpp5+2SpUqWbZs2axIkSL23nvv2YwZMywsLMwKFSpklSpVskqVKlmZMmXS7Li/+fPnW8aMGa1o0aIWGBhouXLlcrcexZs5c6aVKVPGnnvuOY/W/bRsyZIl9sILL1ilSpWsXr16CQ5qGzRoYCEhIfbFF1+k6JN7idW8eXNbtGiR+/6PP/5opUuXtuzZs9u0adM8tp07d67VqlXL6tat69F1+rfffrMBAwZYeHh4iuuu+Ntvv1njxo09us4dPXrUOnfubLly5bL27dt7bH/x4kX773//awUKFHC3nKWV7+pbISilcPE/qPPnz7csWbJY9+7dbfz48ZYvXz6LiIhwD+yMjY21zz//3MqXL29169ZNcQdnd0v8l8DevXttz5499tdff5mZ2enTp61w4cJWoUIFd0iIH4CeN2/eBGN8UqJx48ZZxYoVrUWLFnb48GHr3r27BQYG2uLFi23fvn125swZq1Wrlj355JN8Wd7C8ePHrVSpUvbBBx/Y5MmTzeVyuc/UHThwwHr16mUBAQHuizxeuHDBXnzxRatcuXKKmwDkq6++8uiqu3XrVtu0aZP7frNmzczlclmpUqWsTp061qpVK2vevLn7fZTSXu8/MWXKFMuaNatt2rTJTp8+bUePHrVatWpZ1apVbfLkyXbw4EEbNmyYDR061CZOnOj+Lk8LB7TXf5+cOXPGBg8ebJMmTbLY2FhbsGCB1axZ0ypWrGi//vqrx+Pmzp2bIKCnVatXr7b06dNbkyZNrGTJkubv728ffPBBgpNaVatWtYIFC6a569/8+eef9vrrrye4iPfYsWOtcOHCVq1atQSt3p9++qmVKVPGo0Xz8uXLtnHjxhTbQh7/fli7dq07AJ44ccJ69+5tpUqV8hiWYXbt92ns2LFpZpKd20FQSoGmT59uH3zwgfv+r7/+asWKFXP3gT9y5IiFhYVZpkyZrFixYu5BsGbXzhiktS/Mv/Ppp59agQIFLGfOnFa/fn33maYffvjBSpYsaUFBQRYREWE1a9a0HDlypLizSrcydepUe/zxx+2ZZ56xn376yfr162f+/v6WJ08ee+ihh6x06dLuHxrC0s3169fPQkJCzOVyeXw2zW4cli5evJjiutsdO3bMwsPDrV27dvbzzz9bdHS0hYaG2jPPPONx4uX555+3gIAA++yzz7xXbDIwaNAgq1y5ssXGxrq7Ph08eNDKlStn+fPnt3nz5rm3jf9spfaWpMWLF3vc37Rpk+XOndsqVKhg69evdy9fsmSJ1a1b1ypUqJCgZQnXZmvr37+/+zff7Fp38aJFi9rkyZMT/Man9ZnLxo0b5540xezaeMmKFStamzZtEswy+u2337o/ryn5N+/62k+cOGFPPPGEFStWzH2y6tixY9ajRw8rX758grCUkl/33UBQSmEuXLjgPtsW38d269atNmDAAIuLi7NDhw5Z/vz57cUXX3RfFLRGjRqpfuB0Yu3Zs8fy589v48aNs1mzZlnLli3t4YcftvHjx5vZtW6Lw4YNs759+9obb7zhETpTsuu/CCdPnmxVq1a1Zs2a2enTp+3nn3+2efPm2bx589LUWe7EiP9B3bZtm2XOnNly5Mhh48aNSzBByoEDB+yVV14xl8vlnq4/Jdq0aZOVL1/eOnToYGfOnLGVK1fagw8+aG3btvU4gfDMM89Yzpw5bebMmWnu4oTxn63XXnvNypYt63798SccVq5caZkyZbKIiAibNWuWx2NSs3Xr1lnWrFnt6NGj7tf7008/WYMGDczX1zdBt9+lS5dagwYNrHDhwnQTv86OHTusUqVKli9fvgRdelu2bGlFihSxqVOnuq+Dl9adOHHCWrZsafnz5/fYX2PGjLHHHnvM2rRpY0eOHEnwuNQ2rmvBggXWoEEDq1ChgrtlKT4sPfbYY6li2vO7haCUAh05csSaNm1qVatWdYelPXv2WGxsrLurS3R0tF25csVq1aplLpfLKlasmKAJOq3bsmWL9e/f37p37+7+4f7jjz/spZdestKlS9u7777r5QrvLmdYqly5sjVr1szd5J5WznInhePHj9umTZts4MCBFh4ebqNGjUoQlk6ePGl9+/ZN8Qd9mzdvttKlS1v79u3t9OnT9v3331tYWFiCsFS/fn0LDw9Pswds27Zts/Tp09uQIUM8lsdfu6R69epWs2bNNDPeJjo62j1D6PVjJjZt2mS1a9e+4UD5hQsXWtOmTW3v3r33stRkr0uXLpYtWzZr3bp1gs9XmzZtLCgoyD0eMK25UcDZsmWLdenSxYoUKeIxNundd9+1KlWq2L/+9S/3xEYpXUxMjMfFZK//flm0aJG7pfb6sNShQwerWbNmiuvlcK8QlFKQuLg4d9jZvn271a1b1ypWrOjuzmNmVqlSJY+uP127drU1a9ak6OmH74Zz585Z06ZNLUeOHPbkk096rIsPS+XKlbPhw4e7l6fGHx1nWHr88cc9whJu7Pqxbbt27XJfSNXMrFevXhYeHm7vvPOOOyy9/fbbqepg7/qwdObMGY+wdH03vJTarz+pTJkyxTJkyGB9+/a1jRs32p49e6x+/fr25ptv2o4dO8zlctmKFSu8XeZdc6OD1r1795qPj4/16tXLvWzTpk1Wr149CwsLSxCW6Cp+Y3369LESJUrYsGHDEkzi0LFjxxR7bbZ/4vr32969ez2Oe7Zu3WovvviiFS5c2CMsvfnmm9apU6cU34LknGDqyy+/tCeeeMLq16/vcRxzfbfW+G54f/31lx07duye1puSEJRSkPiDszlz5tgzzzxjFStWNH9/fytQoIBNnTrVzMwef/xxq1KlivtaFKGhoWn+YOVm1q1bZ88884wFBQV5hE2zay10bdu2tSpVqiSYGju1uT4sTZ061apUqWL9+vWzy5cvp8pwmFSuH9v25JNPelwTp3fv3pY/f37r2LGjvfjii+ZyuTym004NbtSy9OCDD9rTTz/tPluZ1t8/cXFxNm/ePAsKCrI8efLY/fffb2XKlLGoqCjbt2+fFSxYMMHFiVObAwcO2Ny5c83M3N2bx44da/7+/jZw4ED3dhs3brR69erZgw8+yLgkh1WrVlmfPn2sXbt2Hj0devToYY888oi98cYbTON8nYEDB9oDDzxg+fPnt9q1a7un+v7ll1/sxRdftKJFi3p0w4v/nkqpYWnr1q3mcrncn6eVK1eav7+/dezY0Vq3bm1+fn7Wrl079/ZLliyxJ5980goXLpxgwhQkRFBKYdavX2+ZMmWySZMm2e+//267d++2atWqWfny5W3u3Lm2ZcsWK168uIWFhVn+/PlT1cQD/0T8F+HZs2ft3Llz7i/E7du3W5MmTaxKlSoJxo78+eefaeZCa9cf0Pbp08cqV66cZroEJcaNxrY98sgjNnr0aPc2r7/+ujVq1MiqVq2aag+GnS1LK1eutOLFiycYIJ3WHTp0yNatW2ffffed+7unf//+VqRIkVT9HXPlyhVr3ry5VapUyXr27Gkul8vdXXzixInm4+PjEZY2b95slStXtuLFi9uVK1fSfNA2M/v8888tMDDQWrZsaa+++qq5XC5r0aKF++C/e/fu9uijj9rAgQPT7GUcrg848+bNs5CQEJszZ47973//s2LFilnx4sXd45B++eUX69y5s2XLls1jcpGU/F67fPmyffTRR5YxY0YbMmSILVy40EaNGmVm18YXL1261LJkyWJt2rRxP2bBggV0a71NBKUUZsKECVakSBGP7giHDh2yypUrW8GCBW3evHl2+fJl27t3b6rpc/tPxX8Bfvnll1a5cmUrVaqUFStWzGbNmmVXr161rVu32jPPPGOPP/64x0xUaU38fhoyZIg9+OCDdvbsWS9XlDzdamxbmTJlPMLSuXPn3NfiSq02b95sZcuWtWeeecbOnj1rly5d8nZJydqvv/5qzz33nOXIkSNNXKbhzJkzVqFCBXO5XNa5c2f38qioqBuGpa1bt6b5Wdri7d+/3woXLuye3e78+fOWNWtW69mzp8fY0Xbt2llERESa/82fNWuWTZ8+3SZOnOhetnfvXnv44YetWLFi7pMSmzdvtrfeeitFj7+9UevXhx9+aBkzZrRcuXJ5/A6ZXRsbGRAQ4HHtJLq13h6CUgozffp0K1iwoLs/afyYpW3bttl9991nxYoVs08++cSbJSZLixcvtsyZM9uIESPs999/t2effdbuu+8+++6778zsWrePZ5991kqUKGHz58/3brFeFBcXZ3Pnzk113cSSSmLGtqUFP/30k1WpUuWGs0fh/1y9etU2b95svXv3TjNdXq5cuWLVq1e30qVLW61atTx+ny5dumQTJ040f39/6969u/eKTKZ27txp5cuXN7NrB/yhoaHWsWNH9/qffvrJ/f9pcYzJ9WHhwIED9sADD5jL5bK33nrLY7u9e/faI488YiVLlkwwFCElh6Xru7XOmTPHWrRoYZMmTbLAwEDr0KFDgu2XL1+e4IQF/h5BKYXZvXu3ZcyY0f797397LN+4caNVqVLFnn322TQ/ccP1Z/DjJ8Bo0qSJ+6zlsWPHrGDBgvbiiy96PG79+vXWrl07LmiIW2Js242ltWnA/4m0NgPp5cuX7ejRo1a/fn2LiIiwjz/+2GP96NGjLXfu3AlmikzrNm/ebOHh4bZgwQLLly+fdezY0X2phq1bt1qNGjXSRKvk3+nfv7/16tXLvv/+eytTpow9+uij7q7j8a3++/btszx58lirVq28WWqSub5ba48ePdzdWuPi4mzSpEmWIUMGe/XVVxM87ptvvkkwYQpujaCUAn388ceWIUMGGzhwoP355592+vRpe/XVV61NmzZpfkBn9+7dbciQIR5nmq5evWoVKlSwH374wc6ePWshISEeZ+VmzJjh7qfLwR6ux9g2IOnEz/pXo0YN92D6//znP9amTRv31OFp3Y4dO2zNmjX2559/WmxsrD3zzDN23333WaNGjTy2GzhwoFWqVClNtiRdP55oyZIlVqhQIfvxxx8tNjbW1q9fb+Hh4Va9evUEF449evRoim5Bcrqdbq03Cku4My4zMyFFMTPNmjVLL774onLmzKl06dLp7NmzWrFihR5++GFvl+dVs2bNUpEiRVSmTBnFxMTIx8dHktSgQQP5+vpq8+bNqlevnt555x35+vrqwoULatmypWrWrKmXXnpJkuRyubz5EpBMmJlcLpe++uorjRw5UufPn1dMTIxeffVVNWnSRNu3b9ewYcN09OhRvfzyy2rSpIm3SwaSvb1796p3797avXu3MmbMqN27d2vZsmWqUKGCt0vzui+++EKtWrVScHCwDh06pIkTJyo6OloffPCBwsLC1LFjR/n7++vLL7/UlClT9N1336lkyZLeLttrFi9erAULFigoKEivv/66e/mPP/6oZs2aqUCBAlqxYkWC3/TY2FilT5/+Xpeb5K5evao6dero9OnTypUrl9q0aaOWLVtKkqKiojRz5kx169ZNnTp10ujRo71cbQrm3ZyGf2Lv3r22YMECmz17NjOXOCxatMj69+/vPts2c+ZMCw8Pt9KlS3tsN3DgQCtQoADXDcINMbYNSHqHDh2ySZMm2dChQ+kGZNdaPE6fPm2PPfaYTZgwwXbv3m2vv/66+fj42AcffGDjxo2zZs2amb+/v5UoUcIqV66c5seRHjlyxEqXLm0ZM2a01q1bJ1j/448/Wv78+a1EiRIpeka7v0O31ruPoIRUadKkSeZyuezVV1+1ixcv2vnz561fv35WtGhRq1WrlvXt29eaNWtm2bJlYwp1mBlj2wB4R1RUlF26dMkGDhzoMbZx1KhR5uPjY2PGjLG//vrL9u/fb6dOnWJG0v9vy5YtVr16dStQoIB99tlnCdZ/99139vTTT6eq7nY3Q7fWu4eud0gV7P93k9q/f79CQkLk6+urjz/+WG3atFHfvn31xhtv6PLly1q+fLmmT5+uK1euKH/+/OratauKFCni7fLhZT169FC2bNn073//W+nSpZMkxcTEqHLlyho9erSKFSumokWLqkGDBpowYYIkaebMmapUqZLy5s2ry5cvK2PGjN58CQBSoAULFmj8+PE6cOCAzExz5szx6E43ZswY9evXT3369FG/fv2UJUsWL1brHXFxce7v5ev/X5I2bdqkPn36KFOmTOrSpYvq169/w+dILd3tboVurXcHQQkpXnxIWrBggYYPH64mTZqoZ8+eSp8+vaZPn662bduqb9+++ve//63MmTN7u1wkQ4xtA3Cvbdy4UTVq1FDLli0VFRWlGTNmqEuXLurZs6fCw8Pd240cOVIjR47U7t27lSNHDi9WfO9dH4w+/PBDbd26VZGRkWrSpIlq1aqlgIAA/fjjj+rfv78yZ86sLl26qF69el6u2nsOHz6sZcuW6dChQ2rWrJkKFy7s7ZJSPIISUoWvvvpKTZo00ejRoxUREaGiRYu618WHpUGDBumll15S7ty5Jf1fwALiLV68WGvWrFGPHj2UO3duzZo1SwMGDFC2bNm0ZcsW93aDBg3S3LlztXz5cuXLl8+LFQNIifbs2aPp06fL399f/fv3lySNHz9ew4YNU6tWrdSpUyePsHTmzBlly5bNW+V6Xf/+/TVp0iS1b99eO3fu1OHDh1WtWjW9+uqrCgwM1I8//qiBAwfq4sWLeuedd1SxYkVvl4xUwsfbBQD/VGRkpN577z0NGDBAXbp0cS+/evWqMmTIoNatW8vlcqlNmzby9fXVoEGDlC5dOkISEjh27JhGjhwpHx8fDRgwQA0aNNDPP/+shQsXqnbt2ipTpoz279+v5cuX65tvviEkAbhjkZGRat68ufbt26eOHTu6l3fu3FlxcXEaPny40qdPr+eff979HZM1a1YvVet9U6dO1aeffqply5bp4Ycf1pdffqmnnnpKUVFRunz5st58801VqFBBQ4YM0dy5c+lqhiRFUEKKFx0drd9//z3B9MwZMmSQdG2syXPPPaf06dOrdOnSHv2bkbY5x7a1b99eGTJkUJs2bXT16lW98cYbevXVV1W+fHlNnz5dv/zyi/Lnz6+1a9cytg1AomTJkkUfffSRmjVrptWrV+vXX39V8eLFJUldu3ZV+vTp1bNnT/n6+mrgwIHy8fFJ0yf2rly5oueee04PP/ywvvjiC7Vv317vvPOODh06pEmTJsnX11evvvqqHn/8cT3++OOSEo5lAhKLrndIceIPbrdu3aocOXIoMDBQ1atXV/PmzdWnTx+PbTdv3qwVK1aoV69e7uAESIxtA+Bd27ZtU5s2bVS+fHm9/PLLKlasmHvdpEmTVKVKFRUsWNCLFd57N+oSHxkZqQsXLsjlcqlevXpq1aqVevfurcOHD6tcuXLy8fFRt27d9Morr9ClHkmOuI0UJf5L8IsvvlD9+vX10UcfKUuWLCpXrpxGjhypdevW6frs/9lnn+nrr7/WhQsXvFg1kqP4i8k2a9ZMrVu3Vv369d2zIrVu3VpTp07VW2+9pREjRuivv/5yP45zSwCSQsmSJTV58mRt3LhRY8aM0Y4dO9zrnn/++TQXkuLi4twh5+LFizp+/Likay1woaGh2rVrl86dO6e6detKko4fP67KlSvr3//+t3r37i2JSXWQ9Oh6hxTF5XJp0aJFatGihcaOHas6depIujYI9uTJk2rYsKG6dOkif39/7dmzR7Nnz9b333+fpgfB4sYY2wbA28qUKaOJEyeqU6dOev311zV48OA02a3XzNxd5V5//XV9//33+umnn9SyZUs9+uijatWqlTJmzKhMmTLpyy+/VLp06fSf//xHOXPmVIcOHeRyudLEFOC49+h6hxTl8uXLat26tQoWLKg333xTly5d0uHDh7Vw4UKVLFlSH330kaKjo7V3714VLFhQQ4cOVYkSJbxdNpKhEydOqGzZsnr11Vf1wgsvJFgfP034zJkzVbp0aT300ENeqBJAWrBhwwa98sormjVrlkJCQrxdjtf85z//0Ycffqj33ntPuXPnVs+ePd1dpHPkyKHevXtr+fLlioqKUlhYmL7//ntlyJCBLne4a2hRQopiZtq7d6+Cg4N1+vRpDR48WNu2bdOuXbvk5+enbt26qUuXLu6DXH9/f2+XjGTiRmPbcuXKpXPnziXY9vqxbS1atPBCtQDSknLlymnp0qVp6sLV48ePV6VKlVSqVCmZmfbs2aPFixdr5syZqlmzpr7//nv9/vvvGjdunMLCwiRJ7777rnbv3q3Tp0+rUqVKSp8+vce174CkxhglpCj+/v7q1q2bJk6cqHz58unw4cN6/vnndfToUTVs2FCLFi1ShgwZFBAQQEiCG2PbACR3aSkk7d27V8OGDdO4ceO0Y8cOuVwuZciQQVFRUapWrZo+//xz1a1bV++8847atWunqKgozZ49WydPnlSxYsX0+OOPK3369IqNjSUk4a7i3YUUp3Xr1ipbtqwOHz6sWrVqKS4uTpIUGxursLAwvjiRAGPbACD5yJcvnxYuXKiOHTvqnXfeUc+ePZUjRw6dOHFCgwcP1vjx4zVixAh16tRJkvTbb79pxowZeuCBBxQaGup+HsYk4W5jjBJSvN9//10ff/yxPvjgA33//ffu61EA8RjbBgDJz5YtW9ShQweVKVNGQ4cO1YwZM9S/f391795d77zzjiQpKipKTZs2VVxcnL766iuuj4R7itPuSNE2bdqkUaNGaevWrVq9ejUhCTfE2DYASH7iZ/17/vnnNXToUNWrV0/dunXTu+++q/Tp0+vq1avavn27/vrrL23evFnp0qXjYrK4p2hRQooWFRWljRs3Km/evO7BnsCNTJ8+XZ06dVKGDBlUo0YNPfXUU2rdurW6d++uX375RcuXL6fLJgB4wZYtW9SxY0c98sgjatGihf78809NmzZN2bNnV8GCBfXGG2/Ix8eHiRtwzxGUAKQZO3bs8Bjbli5dOr300ks6f/68PvroI/n5+Xm7RABIkzZv3qyOHTvq4Ycf1tChQxNMk851kuANBCUAaRJj2wAgedmyZYteeOEF5c2bVyNGjFCBAgUkieskwWvo5Akgzdm0aZNee+01zZ8/n7FtAJBMlClTRuPGjVOWLFn04IMPupcTkuAttCgBSHMY2wYAyVd8CxITN8DbCEoAAABIVuhuh+SAmA4AAIBkhZCE5ICgBAAAAAAOBCUAAAAAcCAoAQAAAIADQQkAAAAAHAhKAAAAAOBAUAIA4Aby5s2rMWPGeLsMAICXEJQAAMnShx9+qICAAMXExLiXXbhwQRkyZNDjjz/use2aNWvkcrm0a9eue10mACCVIigBAJKliIgIXbhwQRs3bnQvW7NmjYKDg7VhwwZdunTJvXzVqlUKDQ1VoUKF7uhvxMbGKi4uLslqBgCkHgQlAECyVLhwYYWGhmrVqlXuZatWrVLDhg2VP39+rV271mN5RESEzpw5o9atWytbtmzKlCmT6tatq927d7u3mzp1qrJmzaqvvvpKDz30kPz8/LR//34dP35cDRo0kL+/v/Lly6cZM2YkqGfIkCF64IEH5Ofnp9DQUL388st39fUDALyLoAQASLaqVaumlStXuu+vXLlS1apVU9WqVd3Lr1y5onXr1ikiIkJt27bVxo0btXDhQq1bt05mpnr16unq1avu57h06ZKGDx+uiRMnavv27QoKClLbtm21b98+ffvtt/r00081btw4HT9+3P2YTz/9VO+8844mTJig3bt364svvlCJEiXu3Y4AANxzPt4uAACAm6lWrZp69uypmJgYRUVFacuWLapSpYpiY2M1duxYSdL69esVFRWlypUrq0OHDvrhhx9UqVIlSdKMGTMUFhamL774Qk2bNpUkXb16VePGjVOpUqUkSbt27dKSJUu0fv16VahQQZI0adIkFS1a1F3HgQMHFBwcrJo1aypDhgx64IEHVL58+Xu5KwAA9xgtSgCAZCsiIkIXL17Uhg0btGbNGhUqVEhBQUGqWrWqNmzYoIsXL2rVqlV64IEHtHPnTvn4+LjDjiTlyJFDhQsX1m+//eZe5uvrq5IlS7rv//bbb/Lx8VHZsmXdy4oUKaKsWbO67zdt2lRRUVF68MEH9cILL2j+/Pkek0wAAFIfghIAINkqUKCA8uTJo5UrV2rlypWqWrWqJCk4OFj58uXTDz/8oJUrV6p69eoysxs+h5nJ5XK57/v7+3vcj3/c9cucwsLCtHPnTn3wwQfy9/dXly5dVKVKFY8ufQCA1IWgBABI1iIiIrRq1SqtWrVK1apVcy+vWrWqli1bpvXr1ysiIkIPPfSQYmJi9OOPP7q3OXXqlHbt2uXRjc6paNGiiomJ8Zhdb+fOnTp79qzHdv7+/vrXv/6lsWPHatWqVVq3bp1++eWXJHudAIDkhTFKAIBkLSIiQl27dtXVq1fdLUrStaDUuXNnXb58WREREQoLC1PDhg31wgsvaMKECQoICFD//v11//33q2HDhjd9/sKFC6tOnTp64YUX9NFHH8nHx0c9evSQv7+/e5upU6cqNjZWFSpUUKZMmfTxxx/L399f4eHhd/W1AwC8hxYlAECyFhERoaioKBUoUEC5c+d2L69atarOnz+v/PnzKywsTJI0ZcoUPfLII3ryySdVsWJFmZkWL16sDBky3PJvTJkyRWFhYapataoaN26sjh07KigoyL0+a9as+t///qfHHntMJUuW1DfffKMvv/xSOXLkuDsvGgDgdS67WaduAAAAAEijaFECAAAAAAeCEgAAAAA4EJQAAAAAwIGgBAAAAAAOBCUAAAAAcCAoAQAAAIADQQkAAAAAHAhKAAAAAOBAUAIAAAAAB4ISAAAAADgQlAAAAADAgaAEAAAAAA7/D3bFWXZrmIvRAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MResNLUxQ1JW"
   },
   "source": [
    "## Part 4: Topic Modeling with LDA - Uncovering Hidden Themes\n",
    "\n",
    "Imagine you have thousands of Reddit comments, news articles, or customer reviews. How can you quickly understand the main topics being discussed without reading everything? **Topic Modeling** helps us do just that!\n",
    "\n",
    "It's an **unsupervised machine learning** technique that analyzes a collection of documents (our \"corpus\") and automatically identifies clusters of words that tend to appear together, representing underlying **topics** or themes.\n",
    "\n",
    "**Latent Dirichlet Allocation (LDA)** is one of the most popular algorithms for topic modeling.\n",
    "\n",
    "**The Core Idea of LDA:**\n",
    "1.  It assumes each **document** is a mixture of different **topics**.\n",
    "2.  It assumes each **topic** is a mixture of different **words**.\n",
    "3.  LDA's goal is to figure out these mixtures: which topics are present in each document, and which words are important for each topic.\n",
    "\n",
    "Think of it like sorting a pile of mixed LEGO bricks: LDA tries to figure out which bricks (words) belong to which type of model (topic), and how many models (topics) are represented in each sub-pile (document).\n",
    "\n",
    "**Applications:**\n",
    "*   Summarizing large text datasets.\n",
    "*   Discovering themes in customer feedback or social media.\n",
    "*   Organizing scientific articles or news reports.\n",
    "*   Recommending content based on thematic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cNDgP53RLAi"
   },
   "source": [
    "### Text Preprocessing: Cleaning the Data for LDA\n",
    "\n",
    "LDA works best with clean, processed text. Common steps include:\n",
    "\n",
    "1.  **Tokenization:** Splitting text into individual words or tokens.\n",
    "2.  **Lowercasing:** Converting all text to lowercase to treat \"Topic\" and \"topic\" the same.\n",
    "3.  **Removing Stop Words:** Eliminating common words (like \"the\", \"is\", \"in\", \"și\", \"cu\") that don't carry much specific meaning.\n",
    "4.  **Removing Punctuation/Numbers:** Often removes non-alphabetic characters unless they are specifically relevant.\n",
    "5.  **Stemming / Lemmatization:** Reducing words to their root form.\n",
    "    *   **Stemming:** Chops off word endings (e.g., \"running\" -> \"run\", \"studies\" -> \"studi\"). It's faster but can sometimes create non-words. (Uses algorithms like PorterStemmer).\n",
    "    *   **Lemmatization:** Considers the word's context and dictionary form (lemma) to find the root (e.g., \"running\" -> \"run\", \"studies\" -> \"study\", \"better\" -> \"good\"). It's usually more accurate but slower. Requires knowledge of the word's part-of-speech (verb, noun, etc.). (Uses tools like WordNetLemmatizer).\n",
    "\n",
    "Lemmatization is often preferred for topic modeling as it produces actual words, making topics more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5w2j3dw0QyUg",
    "outputId": "c5ef5247-a31e-473a-8788-ddf1d3df4798"
   },
   "source": [
    "# @title Stemming vs. Lemmatization Example\n",
    "\n",
    "import nltk\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger') # Needed for POS tagging for lemmatization\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK data (punkt, wordnet, averaged_perceptron_tagger)...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet # To get part-of-speech for lemmatizer\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Helper function to map NLTK POS tags to WordNet POS tags\n",
    "def nltk_pos_to_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "words_to_process = [\"running\", \"studies\", \"better\", \"programming\", \"computers\", \"historical\"]\n",
    "\n",
    "print(\"--- Stemming vs. Lemmatization ---\")\n",
    "\n",
    "for word in words_to_process:\n",
    "    # Stemming (simple)\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "\n",
    "    # Lemmatization (needs Part-of-Speech)\n",
    "    # 1. Get the NLTK POS tag\n",
    "    nltk_pos_tag = nltk.pos_tag([word])[0][1]\n",
    "    # 2. Convert to WordNet POS tag\n",
    "    wordnet_tag = nltk_pos_to_wordnet_pos(nltk_pos_tag)\n",
    "    # 3. Lemmatize with the POS tag (default is noun if no tag or unknown)\n",
    "    if wordnet_tag is None:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word) # Default to noun\n",
    "    else:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=wordnet_tag)\n",
    "\n",
    "\n",
    "    print(f\"Original: {word:<15} | POS Tag: {nltk_pos_tag:<5} | Stemmed: {stemmed_word:<15} | Lemmatized: {lemmatized_word:<15}\")\n",
    "\n",
    "print(\"\\nNotice how lemmatization produces real words ('study', 'good') while stemming can be cruder ('studi', 'better').\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPo1kwzSRXjU",
    "outputId": "bdbbf8ac-5365-41c3-f6e5-05970b10d637"
   },
   "source": [
    "# @title Fetching Example Documents (Wikipedia Articles)\n",
    "\n",
    "# For demonstrating LDA, let's grab content from a few distinct Wikipedia pages.\n",
    "!pip install wikipedia-api scikit-learn --quiet\n",
    "import wikipediaapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords data...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Set up Wikipedia API\n",
    "# Using a custom user agent is good practice\n",
    "# --- CORRECTED User-Agent ---\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "    user_agent='TimisoaraDataMiningLab/1.0 (educational)'\n",
    ")\n",
    "\n",
    "# Define article titles for different topics\n",
    "article_titles = {\n",
    "    \"Religion & Philosophy\": [\"Religion\", \"Stoicism\"],\n",
    "    \"Science & Technology\": [\"Artificial intelligence\", \"Quantum computing\"],\n",
    "    \"Arts & History\": [\"Mona Lisa\", \"History of Romania\"],\n",
    "    \"Nature & Geography\": [\"Carpathian Mountains\", \"Danube Delta\"]\n",
    "}\n",
    "\n",
    "documents_dict = {}\n",
    "print(\"Fetching Wikipedia articles...\")\n",
    "\n",
    "for category, titles in article_titles.items():\n",
    "    print(f\"  Category: {category}\")\n",
    "    for title in titles:\n",
    "        page = wiki_wiki.page(title)\n",
    "        # The error occurred on the next line because the request triggered by page.exists()\n",
    "        # included the problematic User-Agent header.\n",
    "        if page.exists():\n",
    "            print(f\"    - Fetched '{title}' ({len(page.text)} chars)\")\n",
    "            documents_dict[title] = page.text\n",
    "        else:\n",
    "            print(f\"    - Could not find page '{title}'\")\n",
    "\n",
    "# Combine into a list for processing\n",
    "documents = list(documents_dict.values())\n",
    "document_names = list(documents_dict.keys()) # Keep track of names\n",
    "\n",
    "print(f\"\\nFetched content for {len(documents)} articles.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50UW7ImzR3ii",
    "outputId": "f33538af-798c-4cd4-c6e1-74f17dba010d"
   },
   "source": [
    "# @title Preprocessing the Text Documents\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"Downloading necessary NLTK data...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add some common wiki markup/noise to stopwords\n",
    "stop_words.update(['ref', 'cite', 'name', 'http', 'https', 'www', 'com', 'org', 'html', 'also', 'may', 'use', 'first', 'one', 'see', 'page', 'article'])\n",
    "\n",
    "# Helper function to get WordNet POS tag (from Cell 26)\n",
    "def nltk_pos_to_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'): return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'): return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'): return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'): return wordnet.ADV\n",
    "    else: return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # 3. Remove wiki markup like [[File:...]], {{...}}, ==...== etc.\n",
    "    text = re.sub(r'\\[\\[(?:[^\\]|]*\\|)?([^\\]|]*)\\]\\]', r'\\1', text) # Keep link text [[Category:Foo|Bar]] -> Bar\n",
    "    text = re.sub(r'\\{\\{[^}]*\\}\\}', '', text) # Remove templates {{...}}\n",
    "    text = re.sub(r'==\\s*[^=]*\\s*==', '', text) # Remove section headings == ... ==\n",
    "    text = re.sub(r\"\\'\\'\\'?|\\'\\'\", '', text) # Remove bold/italic ''', ''\n",
    "    # 4. Remove punctuation and numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "    # 5. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # 6. Get POS tags\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # 7. Lemmatize and remove stopwords/short words\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(word, pos=nltk_pos_to_wordnet_pos(tag) or wordnet.NOUN) # Default to noun if POS unknown\n",
    "        for word, tag in pos_tags\n",
    "        if word.isalpha() and len(word) > 2 and word not in stop_words\n",
    "    ]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "print(\"Preprocessing documents...\")\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# Display snippet of the first preprocessed document\n",
    "print(\"\\n--- Snippet of first preprocessed document ---\")\n",
    "print(preprocessed_documents[0][:500] + \"...\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XdC6ySPRfam",
    "outputId": "f4002922-1ec7-4ec9-9b67-3c79c91f8b46"
   },
   "source": [
    "# @title Creating the Document-Term Matrix (DTM)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer converts text into a matrix of token counts.\n",
    "# max_df: Ignore terms that appear in more than X% of the documents (too common)\n",
    "# min_df: Ignore terms that appear in less than Y documents (too rare)\n",
    "# ngram_range: Consider single words (1,1), or words and pairs (1,2), etc.\n",
    "vectorizer = CountVectorizer(max_df=0.85, min_df=3, ngram_range=(1, 2)) # Consider single words and bigrams\n",
    "\n",
    "# Fit the vectorizer to the preprocessed documents and transform them into a matrix\n",
    "# Rows = documents, Columns = unique terms (words/ngrams), Values = counts\n",
    "dtm = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "print(f\"Document-Term Matrix created with shape: {dtm.shape}\")\n",
    "print(f\"(Documents: {dtm.shape[0]}, Unique Terms/Ngrams: {dtm.shape[1]})\")\n",
    "\n",
    "# Get the actual terms (feature names)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"\\n--- Sample Feature Names (Terms/Ngrams) ---\")\n",
    "print(list(feature_names[:10]) + [\"...\"] + list(feature_names[-10:]))\n",
    "\n",
    "# Example: What's the count of the 100th term in the first document?\n",
    "term_index = 100\n",
    "doc_index = 0\n",
    "if dtm.shape[1] > term_index and dtm.shape[0] > doc_index:\n",
    "  term_name = feature_names[term_index]\n",
    "  term_count = dtm[doc_index, term_index]\n",
    "  print(f\"\\nExample: The term '{term_name}' (index {term_index}) appears {term_count} times in document {doc_index} ('{document_names[doc_index]}').\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "317aBnDCURSL"
   },
   "source": [
    "**Understanding the DTM Parameters:**\n",
    "\n",
    "*   `max_df=0.85`: If a word appears in more than 85% of all documents, it's likely too common across all topics (like \"article\", \"page\" in Wikipedia context) and gets ignored. Helps focus on more distinctive words.\n",
    "*   `min_df=3`: If a word appears in fewer than 3 documents in the entire corpus, it's considered too rare (could be a typo or very niche term) and gets ignored. Reduces noise.\n",
    "*   `ngram_range=(1, 2)`: This tells the vectorizer to consider both individual words (unigrams, like \"data\") and pairs of adjacent words (bigrams, like \"data mining\"). Bigrams can capture phrases with specific meanings (e.g., \"machine learning\").\n",
    "*   *(Implicitly uses `stop_words='english'` if not provided, but we did stop word removal manually earlier)*\n",
    "\n",
    "The resulting `dtm` (Document-Term Matrix) is the primary input for the LDA algorithm. It's typically a **sparse matrix** (mostly zeros) because most words don't appear in most documents."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQ6lUYOSRkT_",
    "outputId": "b5994a1d-b1bf-4393-d0fb-265c2d2d3775"
   },
   "source": [
    "# @title Applying LDA\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import warnings\n",
    "\n",
    "# Ignore some deprecation warnings that might pop up from scikit-learn/pyLDAvis interactions\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# --- LDA Configuration ---\n",
    "# n_components: The number of topics to discover. This is a key parameter!\n",
    "# We know we fetched articles from 4 broad categories, let's try that.\n",
    "n_topics = 4\n",
    "\n",
    "# random_state: Ensures reproducibility. The algorithm has random elements.\n",
    "# n_jobs=-1: Use all available CPU cores for potentially faster training.\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    learning_method='online', # Often faster for large datasets\n",
    "    max_iter=20 # Increase iterations for potentially better convergence\n",
    ")\n",
    "\n",
    "print(f\"Fitting LDA model with {n_topics} topics...\")\n",
    "# Fit the LDA model to our Document-Term Matrix\n",
    "lda_model.fit(dtm)\n",
    "print(\"LDA model fitting complete.\")\n",
    "\n",
    "\n",
    "# --- Displaying Topics ---\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    print(f\"\\n--- Top {n_top_words} words per topic ---\")\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        # model.components_ stores word distributions for each topic\n",
    "        # argsort() gives the indices that would sort the array\n",
    "        # [:-n_top_words - 1:-1] gets the indices of the top N words in descending order\n",
    "        top_words_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_indices]\n",
    "        print(f\"Topic #{topic_idx}: {' | '.join(top_words)}\")\n",
    "\n",
    "n_top_words = 15 # How many words to show per topic\n",
    "print_top_words(lda_model, feature_names, n_top_words)\n",
    "\n",
    "print(\"\\nManual interpretation suggests topics align reasonably well with the source categories.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "O9WKnIz2UcP4",
    "outputId": "f102926d-308b-41fe-ab69-54e38db55840"
   },
   "source": [
    "# @title Visualizing Topics with pyLDAvis\n",
    "\n",
    "# Attempting clean install with latest pyLDAvis\n",
    "!pip install pyldavis --quiet\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import warnings\n",
    "import pandas as pd # Import pandas to check version if needed\n",
    "\n",
    "# Check pandas version being used\n",
    "try:\n",
    "    print(f\"Using pandas version: {pd.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not check pandas version: {e}\")\n",
    "\n",
    "# Enable notebook mode for pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "print(\"Preparing LDA visualization data...\")\n",
    "\n",
    "# --- Check Inputs ---\n",
    "if 'vectorizer' not in locals() or 'lda_model' not in locals() or 'dtm' not in locals():\n",
    "    print(\"ERROR: Missing required objects (vectorizer, lda_model, or dtm).\")\n",
    "    print(\"Please ensure cells [29] and [31] were run successfully after restarting the runtime.\")\n",
    "else:\n",
    "    # --- Compatibility Patch for pyLDAvis and newer scikit-learn ---\n",
    "    patch_applied = False\n",
    "    vectorizer_usable = False\n",
    "    try:\n",
    "        if hasattr(vectorizer, 'get_feature_names_out') and not hasattr(vectorizer, 'get_feature_names'):\n",
    "            # Has new method, lacks old: Apply patch\n",
    "            feature_names_for_patch = vectorizer.get_feature_names_out()\n",
    "            def get_feature_names_compat():\n",
    "                return feature_names_for_patch\n",
    "            vectorizer.get_feature_names = get_feature_names_compat\n",
    "            patch_applied = True\n",
    "            vectorizer_usable = True\n",
    "            print(\"Applied compatibility patch for vectorizer.get_feature_names().\")\n",
    "        elif hasattr(vectorizer, 'get_feature_names'):\n",
    "            # Already has the old method (less likely in new sklearn, but possible)\n",
    "            print(\"Vectorizer already has get_feature_names(). Patch not needed.\")\n",
    "            patch_applied = True # Patch technically not applied, but method exists\n",
    "            vectorizer_usable = True\n",
    "        else:\n",
    "            # Lacks both methods - something is wrong with the vectorizer\n",
    "             print(\"ERROR: CountVectorizer object lacks expected feature name methods.\")\n",
    "\n",
    "    except Exception as patch_error:\n",
    "        print(f\"Error during compatibility patch setup: {patch_error}\")\n",
    "\n",
    "\n",
    "    # --- Create Visualization Data ---\n",
    "    if vectorizer_usable:\n",
    "        try:\n",
    "            print(\"Attempting pyLDAvis.lda_model.prepare()...\")\n",
    "            # Suppress warnings during prepare\n",
    "            with warnings.catch_warnings():\n",
    "                 warnings.simplefilter(\"ignore\") # Ignore all warnings for cleaner output here\n",
    "                 vis_data = pyLDAvis.lda_model.prepare(lda_model, dtm, vectorizer, mds='mmds') # Changed mds to 'mmds' - sometimes more stable\n",
    "\n",
    "            print(\"Visualization data prepared successfully.\")\n",
    "\n",
    "            # Display the visualization\n",
    "            print(\"\\n--- pyLDAvis Interactive Visualization ---\")\n",
    "            display(vis_data)\n",
    "\n",
    "        except Exception as e:\n",
    "             # Print any error that occurs *during* prepare\n",
    "             print(f\"\\n--- ERROR during pyLDAvis.lda_model.prepare ---\")\n",
    "             print(f\"Error Type: {type(e).__name__}\")\n",
    "             print(f\"Error Details: {e}\")\n",
    "             print(\"------------------------------------------------\")\n",
    "             print(\"This might indicate remaining library incompatibilities.\")\n",
    "             print(\"Ensure scikit-learn, pandas, and pyLDAvis versions work together.\")\n",
    "    else:\n",
    "        print(\"Skipping pyLDAvis visualization because the vectorizer object could not be prepared.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 820
    },
    "id": "2a97PAHSbgh7",
    "outputId": "65c3942e-e224-4aac-ff74-501cf88e9f7a"
   },
   "source": [
    "# @title Analyzing Topic Distribution per Document\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Use the fitted LDA model to transform the DTM.\n",
    "# This calculates the probability distribution of topics for each document.\n",
    "# The result 'doc_topic_dist' will have shape (n_documents, n_topics)\n",
    "doc_topic_dist = lda_model.transform(dtm)\n",
    "\n",
    "print(\"Shape of Document-Topic Distribution Matrix:\", doc_topic_dist.shape)\n",
    "\n",
    "# Display the topic distribution for the first few documents\n",
    "print(\"\\n--- Topic Distribution for First 5 Documents ---\")\n",
    "for i in range(min(5, len(documents))):\n",
    "    # Get the probabilities for the i-th document\n",
    "    topic_probs = doc_topic_dist[i]\n",
    "    # Find the index of the topic with the highest probability\n",
    "    dominant_topic = np.argmax(topic_probs)\n",
    "    print(f\"Document: '{document_names[i]}' (Index {i})\")\n",
    "    print(f\"  Dominant Topic: #{dominant_topic} (Prob: {topic_probs[dominant_topic]:.3f})\")\n",
    "    print(f\"  Topic Probabilities: {[f'#{j}:{p:.3f}' for j, p in enumerate(topic_probs)]}\")\n",
    "\n",
    "# Find the most dominant topic overall for each document\n",
    "dominant_topics = np.argmax(doc_topic_dist, axis=1)\n",
    "\n",
    "print(\"\\n--- Dominant Topic for Each Document ---\")\n",
    "for i, doc_name in enumerate(document_names):\n",
    "    print(f\"'{doc_name}': Assigned to Topic #{dominant_topics[i]}\")\n",
    "\n",
    "# You can create a DataFrame for easier analysis\n",
    "topic_df = pd.DataFrame({\n",
    "    'Document': document_names,\n",
    "    'Dominant Topic': dominant_topics,\n",
    "    # Add individual topic probabilities if needed\n",
    "    **{f'Topic {i} Prob': doc_topic_dist[:, i] for i in range(n_topics)}\n",
    "})\n",
    "print(\"\\n--- DataFrame with Dominant Topics ---\")\n",
    "display(topic_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PO4w6Fwbnqc"
   },
   "source": [
    "### Exercise 4: Deep Dive into Reddit Discussions\n",
    "\n",
    "**Goal:** Apply topic modeling and further text analysis techniques to understand discussions across different Reddit communities.\n",
    "\n",
    "**Dataset:** Collect comments from 4 different Reddit posts. Choose posts that:\n",
    "*   Are from **4 distinct subreddits** covering different subject areas (e.g., one tech, one news, one hobby, one discussion-based like AskReddit/RoAskReddit).\n",
    "*   Are reasonably active (e.g., from the 'hot' or 'top' sections, aiming for posts with at least 100-200 comments if possible).\n",
    "*   Fetch **up to 200 comments** (including replies) from **each** post using `submission.comments.replace_more(limit=None)` and `submission.comments.list()`. Aim for a total corpus of at least 400 comments, ideally 600-800.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1.  **Data Collection (PRAW):**\n",
    "    *   Use PRAW to fetch the comments from your 4 chosen posts.\n",
    "    *   For each comment, store: `CommentID` (comment.id), `Subreddit` (comment.subreddit.display_name), `PostTitle` (comment.submission.title), `CommentBody` (comment.body), `Score` (comment.score), `TimeCreated` (convert comment.created\\_utc).\n",
    "    *   Create a single Pandas DataFrame containing all collected comments. Handle potential errors during fetching (e.g., deleted comments might lack an author).\n",
    "2.  **Preprocessing:**\n",
    "    *   Clean the `CommentBody` text: lowercase, remove URLs, remove punctuation/numbers, tokenize, remove stopwords (English + basic Romanian if applicable), and **lemmatize** the words (using POS tagging for better results).\n",
    "3.  **LDA Topic Modeling:**\n",
    "    *   Create a Document-Term Matrix (DTM) from the preprocessed `CommentBody` texts (use `CountVectorizer`, consider `min_df`, `max_df`, and maybe `ngram_range=(1,1)` or `(1,2)`).\n",
    "    *   Apply LDA with `n_components` set to **3, 4, 5, and 8** topics.\n",
    "    *   For each number of topics, print the top 10-15 words for each identified topic. Briefly interpret the topics found for each model setting (e.g., \"With 4 topics, we see themes of X, Y, Z, W\").\n",
    "4.  **Visualization (pyLDAvis):**\n",
    "    *   Choose the LDA model (from the 3, 4, 5, or 8 topic runs) that you think best separates the comments into meaningful, distinct topics based on the top words and your intuition.\n",
    "    *   Generate and display the **pyLDAvis** visualization for your chosen model. Justify your choice briefly.\n",
    "5.  **Topic Assignment:**\n",
    "    *   Using your chosen LDA model, transform the DTM to get the topic distribution for each **comment**.\n",
    "    *   Find the dominant topic for each comment.\n",
    "    *   *Challenge:* Can you determine the overall dominant topic(s) for each of the **original 4 Reddit posts** by aggregating the dominant topics of their comments? (e.g., which topic number appears most frequently among comments for Post 1?). Display this result.\n",
    "6.  **Named Entity Recognition (NER) Analysis:**\n",
    "    *   Apply NER to the original (or minimally cleaned) `CommentBody` text using a library like **spaCy** (`pip install spacy && python -m spacy download en_core_web_sm`).\n",
    "    *   Identify common **Entities** of your choice (like PERSON, ORG - organization, GPE - geopolitical entity/location, PRODUCT, EVENT).\n",
    "    *   Create a visualization (e.g., bar chart) showing the frequency of the top 10-15 most common *named entities* found across all comments.\n",
    "7.  **Investigative Analysis (Your Choice!):**\n",
    "    *   Investigative Analysis: Conduct an additional analysis of your choice on the collected data. This could be an exploration of word trends, PoS trends, a comparison of topic prevalence over time, or any other creative angle that provides additional insights into your corpus.\n",
    "\n",
    "**Hints:**\n",
    "*   Break down the task into functions (e.g., `fetch_comments`, `preprocess_text`, `run_lda`, `perform_ner`).\n",
    "*   Handle potential `AttributeError` when accessing comment attributes (e.g., `comment.author` might be None if deleted). Use `str(comment.author)` or check `if comment.author:`.\n",
    "*   spaCy NER: Load the model (`nlp = spacy.load(\"en_core_web_sm\")`), process text (`doc = nlp(comment_text)`), iterate through entities (`for ent in doc.ents: print(ent.text, ent.label_)`). Collect entity texts and labels for frequency counting.\n",
    "*   Be patient with `replace_more(limit=None)` and LDA fitting – they can take time!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bnB-58D-blxa",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Set up Reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"sIXZTihLNiKiHw\",\n",
    "    client_secret=\"EjfAsmz5z8mDbZohe4UPYTPIZsYmOQ\",\n",
    "    user_agent=\"TimișoaraDataMiningLab\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- Step 1: Data Collection ---\n",
    "def fetch_comments(reddit_post_urls):\n",
    "    data = []\n",
    "    for url in reddit_post_urls:\n",
    "        submission = reddit.submission(url=url)\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        comments = submission.comments.list()\n",
    "        for comment in comments:\n",
    "            if comment.body == \"[deleted]\":\n",
    "                continue\n",
    "            data.append({\n",
    "                \"CommentID\": comment.id,\n",
    "                \"Subreddit\": comment.subreddit.display_name,\n",
    "                \"PostTitle\": comment.submission.title,\n",
    "                \"CommentBody\": comment.body,\n",
    "                \"Score\": comment.score,\n",
    "                \"TimeCreated\": pd.to_datetime(comment.created_utc, unit=\"s\")\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Provide up to four Reddit post URLs here (ensure they cover diverse subreddits)\n",
    "reddit_posts = [\n",
    "    \"https://www.reddit.com/r/leagueoflegends/comments/1dlrjkf/arcane_season_2_official_trailer_november_9/\",\n",
    "    \"https://www.reddit.com/r/gaming/comments/1dljv7u/what_game_did_you_pick_up_after_a_long_time_and/\",\n",
    "    \"https://www.reddit.com/r/AskReddit/comments/1dlpegd/what_is_the_most_underrated_videogame_of_all_time/\",\n",
    "    \"https://www.reddit.com/r/esports/comments/1dlidxn/las_legendary_rekkles_returns_to_competitive/\"\n",
    "]\n",
    "comments_df = fetch_comments(reddit_posts)\n",
    "\n",
    "# --- Step 2: Preprocessing ---\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"\\W\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \" \", text)  # Remove numbers\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "comments_df[\"CleanedCommentBody\"] = comments_df[\"CommentBody\"].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# --- Step 3: LDA Topic Modeling ---\n",
    "def run_lda(data, n_topics):\n",
    "    vectorizer = CountVectorizer(min_df=5, max_df=0.95, stop_words=\"english\")\n",
    "    dtm = vectorizer.fit_transform(data)\n",
    "    lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda_model.fit(dtm)\n",
    "    return lda_model, vectorizer\n",
    "\n",
    "\n",
    "lda_models = {}\n",
    "for n_topics in [3, 4, 5, 8]:\n",
    "    lda_model, vectorizer = run_lda(comments_df[\"CleanedCommentBody\"], n_topics)\n",
    "    lda_models[n_topics] = (lda_model, vectorizer)\n",
    "\n",
    "# Print top words for each number of topics\n",
    "for n_topics, (lda_model, vectorizer) in lda_models.items():\n",
    "    print(f\"\\n--- Top Words for {n_topics} Topics ---\")\n",
    "    for i, topic in enumerate(lda_model.components_):\n",
    "        words = [vectorizer.get_feature_names_out()[j] for j in topic.argsort()[-10:]]\n",
    "        print(f\"Topic {i + 1}: {', '.join(words)}\")\n",
    "\n",
    "# --- Step 4: Visualization (pyLDAvis) ---\n",
    "chosen_n_topics = 4  # Use the number of topics that makes the most sense\n",
    "chosen_model, chosen_vectorizer = lda_models[chosen_n_topics]\n",
    "dtm = chosen_vectorizer.transform(comments_df[\"CleanedCommentBody\"])\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.sklearn.prepare(chosen_model, dtm, chosen_vectorizer)\n",
    "pyLDAvis.display(vis)\n",
    "\n",
    "# --- Step 5: Topic Assignment ---\n",
    "topic_distributions = chosen_model.transform(dtm)\n",
    "comments_df[\"DominantTopic\"] = topic_distributions.argmax(axis=1)\n",
    "\n",
    "# Aggregate dominant topics by post\n",
    "post_topics = comments_df.groupby(\"PostTitle\")[\"DominantTopic\"].agg(lambda x: x.mode()[0])\n",
    "print(post_topics)\n",
    "\n",
    "# --- Step 6: NER (Named Entity Recognition) ---\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def perform_ner(texts):\n",
    "    entities = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        entities.extend([ent.label_ for ent in doc.ents])\n",
    "    return Counter(entities)\n",
    "\n",
    "\n",
    "entity_counts = perform_ner(comments_df[\"CommentBody\"])\n",
    "entity_df = pd.DataFrame(entity_counts.most_common(), columns=[\"Entity\", \"Count\"])\n",
    "\n",
    "# Plot entity frequencies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(entity_df[\"Entity\"], entity_df[\"Count\"])\n",
    "plt.title(\"Named Entity Frequencies\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# --- Step 7: Investigative Analysis ---\n",
    "# Additional analysis: e.g., comparing topic prevalence across subreddits\n",
    "subreddit_topics = comments_df.groupby(\"Subreddit\")[\"DominantTopic\"].value_counts(normalize=True).unstack()\n",
    "print(subreddit_topics)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
